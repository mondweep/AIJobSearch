
Debug: Profile Analysis Flow:

1. Parsing Profile Data...
Categorized 61 technical and 36 soft skills

=== Parsing LinkedIn Data ===

=== LinkedIn Data Parsing ===
LinkedIn Profile Path: /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/linkedin/Profile.csv
LinkedIn Posts Path: /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/linkedin/Shares.csv
LinkedIn Articles Path: /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/linkedin/articles
LinkedIn Endorsements Path: /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/linkedin/Endorsement_Received_Info.csv
Error parsing LinkedIn experience PDF: No module named 'PyPDF2'
Traceback (most recent call last):
  File "/Users/mondweep/DxSure/AIJobSearch/job_search_ai/utils/profile_parser.py", line 533, in _parse_linkedin_experience_pdf
    import PyPDF2
ModuleNotFoundError: No module named 'PyPDF2'

Debug: Successfully opened positions CSV with utf-8 encoding
Error reading positions CSV: I/O operation on closed file.
Traceback (most recent call last):
  File "/Users/mondweep/DxSure/AIJobSearch/job_search_ai/utils/profile_parser.py", line 441, in parse_linkedin_positions
    reader = csv.DictReader(f)
             ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/csv.py", line 91, in __init__
    self.reader = reader(f, dialect, *args, **kwds)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: I/O operation on closed file.

Debug: Starting LinkedIn posts parsing...
Debug: Reading CSV from /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/linkedin/Shares.csv
Error reading posts CSV: I/O operation on closed file.

Debug: Successfully opened certifications CSV with utf-8 encoding
Error reading certifications CSV: I/O operation on closed file.
Traceback (most recent call last):
  File "/Users/mondweep/DxSure/AIJobSearch/job_search_ai/utils/profile_parser.py", line 877, in parse_linkedin_certifications
    reader = csv.DictReader(f)
             ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/csv.py", line 91, in __init__
    self.reader = reader(f, dialect, *args, **kwds)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: I/O operation on closed file.
Degree: Master of Business Administration (M.B.A.)
School: London Business School
Degree: Bachelor of Technology (B.Tech.)
School: National Institute of Technology Calicut
Degree: 
School: London Business School
Degree: Certificate
School: Import Classes

Debug: Successfully opened endorsements CSV with utf-8 encoding
Error reading endorsements CSV: I/O operation on closed file.
Traceback (most recent call last):
  File "/Users/mondweep/DxSure/AIJobSearch/job_search_ai/utils/profile_parser.py", line 400, in parse_linkedin_endorsements
    reader = csv.DictReader(f)
             ^^^^^^^^^^^^^^^^^
  File "/opt/anaconda3/lib/python3.12/csv.py", line 91, in __init__
    self.reader = reader(f, dialect, *args, **kwds)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: I/O operation on closed file.
Debug: Medium path is /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium
Debug: Looking for HTML files in /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium
Debug: Found 18 HTML files
Debug: Found file: /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2019-03-15_Could-AR-rejuvenate-the-high-street--2ecfa098a85c.html
Debug: Found file: /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2019-11-17_New-Ways-of-Working-Transformation-6e17eccf9c5.html
Debug: Found file: /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2019-03-15_100GB-of-monthly-mobile-data-allowance---why-not-arbitrage--c2fe6a19a236.html
Debug: Found file: /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2022-02-02_Oracles-in-Decentralised-Web-d7279687f573.html
Debug: Found file: /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2023-07-21_Causality---Understanding-Counterfactuals--Attribution-and-the-Relevance-to-AI-development-and--aad0f571022b.html
Debug: Found file: /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2019-03-19_Adr--ya---What-if-you-could-simplify-Data-Sharing-Challenges--22f49f66e7ee.html
Debug: Found file: /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2020-12-18_Smart-Energy-Transition-and-Edge-Computing-757c7b6090ab.html
Debug: Found file: /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2022-02-14_Blockchain-basics---Elliptic-Curve-Cryptography-6aa7de17d1c9.html
Debug: Found file: /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2019-07-03_Driving-Agility--Maximising-Value-36a84541fb3a.html
Debug: Found file: /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2022-03-16_Blockchain-basics---NFTs--Non-Functional-Tokens--601601c3637.html
Debug: Found file: /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2023-05-06_Tracking-Electricity-Consumption-data-with-a-ChatGPT-AI-assistant-9f0948e15f02.html
Debug: Found file: /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2019-07-04_Banking-Payment-Systems---Part-1-817e6a8ade49.html
Debug: Found file: /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2018-01-20_When-I-tried-with-an-Europe-region--I-got-an-error-as-shown-below--3622df98f35d.html
Debug: Found file: /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2018-01-20_Hi---thank-you-for-the-article--b3832ed565.html
Debug: Found file: /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2018-01-21_I-connected-the-following-kit-to-my-macbook-and-attempted-the-suggested-command--1394afdf801f.html
Debug: Found file: /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2024-02-09_Cybersecurity-Assessments-22ee38708888.html
Debug: Found file: /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2019-09-22_On-Customer-Experience----Analog--to-Digital-878b2d71ba36.html
Debug: Found file: /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2021-06-27_Making-restaurant-menus-fun-using-AR-c208cfa0e9e6.html
Debug: Processing /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2019-03-15_Could-AR-rejuvenate-the-high-street--2ecfa098a85c.html
Debug: Extracted article: Could AR rejuvenate the high street?
Debug: Processing /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2019-11-17_New-Ways-of-Working-Transformation-6e17eccf9c5.html
Debug: Extracted article: New Ways of Working Transformation
Debug: Processing /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2019-03-15_100GB-of-monthly-mobile-data-allowance---why-not-arbitrage--c2fe6a19a236.html
Debug: Extracted article: 100GB of monthly mobile data allowance — why not arbitrage?
Debug: Processing /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2022-02-02_Oracles-in-Decentralised-Web-d7279687f573.html
Debug: Extracted article: Oracles in Decentralised Web
Debug: Processing /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2023-07-21_Causality---Understanding-Counterfactuals--Attribution-and-the-Relevance-to-AI-development-and--aad0f571022b.html
Debug: Extracted article: Causality — Understanding Counterfactuals, Attribution and the Relevance to AI development and…
Debug: Processing /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2019-03-19_Adr--ya---What-if-you-could-simplify-Data-Sharing-Challenges--22f49f66e7ee.html
Debug: Extracted article: Adṛśya — What if you could simplify Data Sharing Challenges?
Debug: Processing /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2020-12-18_Smart-Energy-Transition-and-Edge-Computing-757c7b6090ab.html
Debug: Extracted article: Smart Energy Transition and Edge Computing
Debug: Processing /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2022-02-14_Blockchain-basics---Elliptic-Curve-Cryptography-6aa7de17d1c9.html
Debug: Extracted article: Blockchain basics — Elliptic Curve Cryptography
Debug: Processing /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2019-07-03_Driving-Agility--Maximising-Value-36a84541fb3a.html
Debug: Extracted article: Driving Agility: Maximising Value
Debug: Processing /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2022-03-16_Blockchain-basics---NFTs--Non-Functional-Tokens--601601c3637.html
Debug: Extracted article: Blockchain basics — NFTs (Non Functional Tokens)
Debug: Processing /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2023-05-06_Tracking-Electricity-Consumption-data-with-a-ChatGPT-AI-assistant-9f0948e15f02.html
Debug: Extracted article: Tracking Electricity Consumption data with a ChatGPT AI assistant
Debug: Processing /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2019-07-04_Banking-Payment-Systems---Part-1-817e6a8ade49.html
Debug: Extracted article: Banking Payment Systems — Part 1
Debug: Processing /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2018-01-20_When-I-tried-with-an-Europe-region--I-got-an-error-as-shown-below--3622df98f35d.html
Debug: Extracted article: When I tried with an Europe region, I got an error as shown below.
Debug: Processing /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2018-01-20_Hi---thank-you-for-the-article--b3832ed565.html
Debug: Extracted article: Hi — thank you for the article.
Debug: Processing /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2018-01-21_I-connected-the-following-kit-to-my-macbook-and-attempted-the-suggested-command--1394afdf801f.html
Debug: Extracted article: I connected the following kit to my macbook and attempted the suggested command.
Debug: Processing /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2024-02-09_Cybersecurity-Assessments-22ee38708888.html
Debug: Extracted article: Cybersecurity Assessments
Debug: Processing /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2019-09-22_On-Customer-Experience----Analog--to-Digital-878b2d71ba36.html
Debug: Extracted article: On Customer Experience — ‘Analog’ to Digital
Debug: Processing /Users/mondweep/DxSure/AIJobSearch/job_search_ai/tests/data/medium/2021-06-27_Making-restaurant-menus-fun-using-AR-c208cfa0e9e6.html
Debug: Extracted article: Making restaurant menus fun using AR

2. Combining Profile Data...

LinkedIn Components Found:
- Profile: Present
- Experience: 0 entries
- Posts: 2 entries
- Articles: 1 entries
- Endorsements: 0 skills
- Certifications: 0 entries
- Education: 4 entries
Combined data keys: dict_keys(['cv', 'skills', 'linkedin', 'medium'])

3. Analyzing Profile...
Debug: Found 7 CV skills
Debug: Found 61 technical skills
Debug: Found 36 soft skills
Debug: Found 0 LinkedIn topics
Debug: Found 23 Medium topics
Analysis result keys: dict_keys(['core_competencies', 'experience_level', 'technical_depth', 'leadership', 'education', 'certifications', 'endorsements', 'content_expertise', 'industry_focus', 'career_progression'])

4. Storing Profile Data...
Final profile_data keys: dict_keys(['raw_data', 'analysis'])
Successfully initialized OpenAI client in JobFilter
JobFilter initialized with OpenAI client successfully
LLM Client Type: <class 'openai.OpenAI'>
OpenAI client has required chat.completions interface
JobFilter initialized with LLM client type: <class 'openai.OpenAI'>
LLM Client Type: <class 'openai.OpenAI'>
LLM Client Attributes: ['__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_base_url', '_build_headers', '_build_request', '_calculate_retry_timeout', '_client', '_custom_headers', '_custom_query', '_default_stream_cls', '_enforce_trailing_slash', '_idempotency_header', '_idempotency_key', '_limits', '_make_sse_decoder', '_make_status_error', '_make_status_error_from_response', '_maybe_override_cast_to', '_parse_retry_after_header', '_platform', '_prepare_options', '_prepare_request', '_prepare_url', '_process_response', '_process_response_data', '_proxies', '_request', '_request_api_list', '_retry_request', '_serialize_multipartform', '_should_retry', '_should_stream_response_body', '_strict_response_validation', '_transport', '_validate_headers', '_version', 'api_key', 'audio', 'auth_headers', 'base_url', 'batches', 'beta', 'chat', 'close', 'completions', 'copy', 'custom_auth', 'default_headers', 'default_query', 'delete', 'embeddings', 'files', 'fine_tuning', 'get', 'get_api_list', 'images', 'is_closed', 'max_retries', 'models', 'moderations', 'organization', 'patch', 'platform_headers', 'post', 'project', 'put', 'qs', 'request', 'timeout', 'uploads', 'user_agent', 'websocket_base_url', 'with_options', 'with_raw_response', 'with_streaming_response']
Successfully initialized OpenAI client in JobFilter
JobFilter initialized with OpenAI client successfully
LLM Client Type: <class 'openai.OpenAI'>
OpenAI client has required chat.completions interface
JobFilter initialized with LLM client type: <class 'openai.OpenAI'>
LLM Client Type: <class 'openai.OpenAI'>
LLM Client Attributes: ['__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_base_url', '_build_headers', '_build_request', '_calculate_retry_timeout', '_client', '_custom_headers', '_custom_query', '_default_stream_cls', '_enforce_trailing_slash', '_idempotency_header', '_idempotency_key', '_limits', '_make_sse_decoder', '_make_status_error', '_make_status_error_from_response', '_maybe_override_cast_to', '_parse_retry_after_header', '_platform', '_prepare_options', '_prepare_request', '_prepare_url', '_process_response', '_process_response_data', '_proxies', '_request', '_request_api_list', '_retry_request', '_serialize_multipartform', '_should_retry', '_should_stream_response_body', '_strict_response_validation', '_transport', '_validate_headers', '_version', 'api_key', 'audio', 'auth_headers', 'base_url', 'batches', 'beta', 'chat', 'close', 'completions', 'copy', 'custom_auth', 'default_headers', 'default_query', 'delete', 'embeddings', 'files', 'fine_tuning', 'get', 'get_api_list', 'images', 'is_closed', 'max_retries', 'models', 'moderations', 'organization', 'patch', 'platform_headers', 'post', 'project', 'put', 'qs', 'request', 'timeout', 'uploads', 'user_agent', 'websocket_base_url', 'with_options', 'with_raw_response', 'with_streaming_response']
Successfully initialized OpenAI client in JobFilter
JobFilter initialized with OpenAI client successfully
LLM Client Type: <class 'openai.OpenAI'>
OpenAI client has required chat.completions interface
JobFilter initialized with LLM client type: <class 'openai.OpenAI'>
LLM Client Type: <class 'openai.OpenAI'>
LLM Client Attributes: ['__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_base_url', '_build_headers', '_build_request', '_calculate_retry_timeout', '_client', '_custom_headers', '_custom_query', '_default_stream_cls', '_enforce_trailing_slash', '_idempotency_header', '_idempotency_key', '_limits', '_make_sse_decoder', '_make_status_error', '_make_status_error_from_response', '_maybe_override_cast_to', '_parse_retry_after_header', '_platform', '_prepare_options', '_prepare_request', '_prepare_url', '_process_response', '_process_response_data', '_proxies', '_request', '_request_api_list', '_retry_request', '_serialize_multipartform', '_should_retry', '_should_stream_response_body', '_strict_response_validation', '_transport', '_validate_headers', '_version', 'api_key', 'audio', 'auth_headers', 'base_url', 'batches', 'beta', 'chat', 'close', 'completions', 'copy', 'custom_auth', 'default_headers', 'default_query', 'delete', 'embeddings', 'files', 'fine_tuning', 'get', 'get_api_list', 'images', 'is_closed', 'max_retries', 'models', 'moderations', 'organization', 'patch', 'platform_headers', 'post', 'project', 'put', 'qs', 'request', 'timeout', 'uploads', 'user_agent', 'websocket_base_url', 'with_options', 'with_raw_response', 'with_streaming_response']
Successfully initialized OpenAI client in JobFilter
JobFilter initialized with OpenAI client successfully
LLM Client Type: <class 'openai.OpenAI'>
OpenAI client has required chat.completions interface
JobFilter initialized with LLM client type: <class 'openai.OpenAI'>
LLM Client Type: <class 'openai.OpenAI'>
LLM Client Attributes: ['__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_base_url', '_build_headers', '_build_request', '_calculate_retry_timeout', '_client', '_custom_headers', '_custom_query', '_default_stream_cls', '_enforce_trailing_slash', '_idempotency_header', '_idempotency_key', '_limits', '_make_sse_decoder', '_make_status_error', '_make_status_error_from_response', '_maybe_override_cast_to', '_parse_retry_after_header', '_platform', '_prepare_options', '_prepare_request', '_prepare_url', '_process_response', '_process_response_data', '_proxies', '_request', '_request_api_list', '_retry_request', '_serialize_multipartform', '_should_retry', '_should_stream_response_body', '_strict_response_validation', '_transport', '_validate_headers', '_version', 'api_key', 'audio', 'auth_headers', 'base_url', 'batches', 'beta', 'chat', 'close', 'completions', 'copy', 'custom_auth', 'default_headers', 'default_query', 'delete', 'embeddings', 'files', 'fine_tuning', 'get', 'get_api_list', 'images', 'is_closed', 'max_retries', 'models', 'moderations', 'organization', 'patch', 'platform_headers', 'post', 'project', 'put', 'qs', 'request', 'timeout', 'uploads', 'user_agent', 'websocket_base_url', 'with_options', 'with_raw_response', 'with_streaming_response']
Successfully initialized OpenAI client in JobFilter
JobFilter initialized with OpenAI client successfully
LLM Client Type: <class 'openai.OpenAI'>
OpenAI client has required chat.completions interface
JobFilter initialized with LLM client type: <class 'openai.OpenAI'>
LLM Client Type: <class 'openai.OpenAI'>
LLM Client Attributes: ['__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__enter__', '__eq__', '__exit__', '__format__', '__ge__', '__getattribute__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_base_url', '_build_headers', '_build_request', '_calculate_retry_timeout', '_client', '_custom_headers', '_custom_query', '_default_stream_cls', '_enforce_trailing_slash', '_idempotency_header', '_idempotency_key', '_limits', '_make_sse_decoder', '_make_status_error', '_make_status_error_from_response', '_maybe_override_cast_to', '_parse_retry_after_header', '_platform', '_prepare_options', '_prepare_request', '_prepare_url', '_process_response', '_process_response_data', '_proxies', '_request', '_request_api_list', '_retry_request', '_serialize_multipartform', '_should_retry', '_should_stream_response_body', '_strict_response_validation', '_transport', '_validate_headers', '_version', 'api_key', 'audio', 'auth_headers', 'base_url', 'batches', 'beta', 'chat', 'close', 'completions', 'copy', 'custom_auth', 'default_headers', 'default_query', 'delete', 'embeddings', 'files', 'fine_tuning', 'get', 'get_api_list', 'images', 'is_closed', 'max_retries', 'models', 'moderations', 'organization', 'patch', 'platform_headers', 'post', 'project', 'put', 'qs', 'request', 'timeout', 'uploads', 'user_agent', 'websocket_base_url', 'with_options', 'with_raw_response', 'with_streaming_response']

Debug - Profile Data Structure:
Complete profile data: {'cv': {'skills': ['Agile', 'Ai', 'Aws', 'Azure', 'Cloud', 'Ml', 'Project Management'], 'experiences': ['Director', 'Engineer', 'Head', 'Lead', 'Program Manager', 'Technical Program Manager']}, 'skills': {'technical': ['Ai For Good Project Framework', 'Amazon Web Services (Aws)', 'Artificial Intelligence (Ai)', 'Artificial Neural Networks', 'Automotive', 'Business Analysis', 'Causal Inference', 'Causality', 'Client Liaison', 'Client Relations', 'Cloud Computing', 'Cyber Security Risk', 'Cybersecurity', 'Cybersecurity Auditing', 'Cybersecurity Tools', 'Data Analysis', 'Delivery', 'Digital Marketing', 'Digital Transformation', 'Entrepreneurship', 'Exploratory Data Analysis', 'Fundraising', 'Generative Ai', 'Google Cloud Platform (Gcp)', 'Google Compute Engine', 'Information Technology Infrastructure', 'Instrumental Variable', 'Integration', 'Jupyter Notebooks', 'Leading Development Teams', 'Machine Learning', 'Machine Learning Algorithms', 'Network Architecture', 'Network Security', 'New Product Validation', 'Partnerships', 'Payments', 'Planning Budgeting & Forecasting', 'Problem Solving', 'Product Design', 'Product Development', 'Program Development', 'Programme Delivery', 'Project Delivery', 'Project Finance', 'Project Planning', 'Propensity Score Matching', 'Requirements Analysis', 'Requirements Gathering', 'Rfp Design', 'Sales Processes', 'Scrum', 'Social Media Marketing', 'Software Development', 'Solution Architecture', 'Stakeholder Engagement', 'Supervised Learning', 'Supportive Services', 'Team Building', 'Technical Support', 'Virtual Machines'], 'soft': ['Agile & Waterfall Methodologies', 'Agile Methodologies', 'Agile Project Management', 'Business Development', 'Business Strategy', 'Change Management', 'Cloud Strategy', 'Coaching', 'Contract Negotiation', 'Cross-Functional Team Leadership', 'Cybersecurity Incident Management', 'Incident Management, Security Information And Event Management (Siem)', 'Information Security Management System (Isms)', 'It Strategy', 'Leadership', 'Management', 'Management Consulting', 'Mentoring', 'P&L Management', 'Product Management', 'Product Strategy', 'Program Delivery Management', 'Program Leadership', 'Program Management', 'Project Management', 'Project Management Office (Pmo)', 'Security Strategy', 'Senior Stakeholder Management', 'Software Project Management', 'Stakeholder Management', 'Strategy', 'Team Leadership', 'Team Management', 'Technical Project Leadership', 'Vendor Management', 'Vulnerability Management']}, 'linkedin': {'posts': {'posts': [], 'topics': []}, 'profile': {'name': 'Mondweep Chakravorty', 'headline': 'I help CIOs at global enterprises achieve revenue growth of at least £15M through targetted digital transformation of their organisations | Head of Technology| Programme Lead', 'location': '', 'industry': 'IT Services and IT Consulting'}, 'education': [{'degree': 'Master of Business Administration (M.B.A.)', 'school': 'London Business School', 'field': 'Finance and Accounting: Corporate Finance, Capital Markets and Financing, Credit Risk, Financial Accounting, Managerial Accounting  Strategy & Marketing:  Strategic Management, Discovering Entrepreneurial Opportunities, New Ventures Development, Brand Communications, Brand Marketing, Energy: Markets, Models and Strategies  Economics: Managerial Economics, Understanding International Macroeconomy, World Economy: Problems & Prospects  Leadership: Developing Effective Managers and Organisations, Leadership Skills, Understanding General Management, Data Analytics for Managers, Decision and Risk Analysis, Operations Management'}, {'degree': 'Bachelor of Technology (B.Tech.)', 'school': 'National Institute of Technology Calicut', 'field': 'Electrical & Electronics Engineering'}, {'degree': '', 'school': 'London Business School', 'field': 'In depth 10 week classroom programme covering valuations, merger and acquisition and project finance.'}, {'degree': 'Certificate', 'school': 'Import Classes', 'field': "Attended import classes' Two-Day Introduction to Data Science Course at London Business School on October 3-4, 2015 - Certificate Number: IDS101502"}], 'articles': {'articles': [{'title': 'Big Data Platform Considerations', 'content': '\n\nBig Data Platform Considerations\n\n\n\nBig Data Platform Considerations\nCreated on 2018-01-11 18:57\nPublished on 2018-02-13 17:58\n\nIn this article, I take the opportunity to summarise the key themes that underlie the need for a big data processing platform; as well as how the components of the Hadoop ecosystem work together to meet those needs.\nThe broad themes that are necessary of a big data platform to be meaningful for business are:\n\nIngestion of different types of data from various sources - traditional file based sources as well as non traditional streaming data like clickstreams, twitter feeds, facebook posts etc\nData discovery - Ability to sift through existing data sources and discover, organise data within the platform for simplified access and utilisation for value added activities\nMetadata capture - Ability to understand the business, operational and technical construct of the ingested data. By this we mean for example whether the received data set is related to healthcare, financials; what frequency the data feeds are expected; and what the expected format of the data is - in terms of number of fields, delimited between various fields etc\nData Quality - Ability to apply quality checks on the data: auto correction where possible, separation of good and bad records etc\nData Transformation & Workflows- Ability to enrich datasets through joins, projections which can then be sewed into a workflow to achieve a desired business outcome\nData Governance - Ability for the platform to apply data lifecycle rules: move data sets around between hot, warm, and cold zones based on pre defined business rules\nData Self Service - Ability for data users to call upon relevant data set using search key words, profile it, combine it with other data sets and validate suitable hypothesis on a sample of the entire data sets. If the hypothesis are statistically valid, then the users should be able to automate the processes and be able to operationalise those on all historical and future data sets\nData Visualisation - Ability for users to visualise the data in interesting ways : primarily graphically.\n\nEvolution of the Key Hadoop technologies\n\n\n\nOn the left is a time chart (courtesy Cloudera) of the key technologies as they have evolved since release of Core Hadoop in 2006. In this instance, The chart is currently limited to 2015. In a subsequent article, I will cover the evolution of server-less or cloud based big data platforms (read hadoop as a service) from the likes of GCP, AWS and Azure.\n\n\n\n\nThe adjacent two tables include a high level description of a few of the key components that make up the Hadoop ecosystem.\nThese are some of the common tools used to manage data pipelines from ingestion to consumption.\nFor example, for ingestion Core Hadoop libraries, Sqoop, Flume, Kafka are used across batch and streaming data.\n\n\n\n\n\nThe ingested data can be stored in HDFS (for example in formats such as csv, tsv, Parquet, or Avro) or in a key value NoSQL database like HBase. The meta data is registered in HCatalog and data indexed using Solr. Users can analyse data using Hive SQL-like interface and implement data quality checks as well as transformation using PigLatin. Workflows can be orchestrated using Oozie. Falcon provides more refined data lifecycle management, disaster recovery and pipeline processing features. Ranger, RecordService provide for enterprise grade security; and YARN, Zookeeper take care of resource management across the Hadoop cluster.\n\n\n\n\nUnderlying all the ecosystem components are the processing frameworks. Whereas MapReduce was originally designed to process massive batch data feeds; Spark, Flink and Tez are other frameworks that can be used to process near real time analytics and data processing.\n\n\n\n\nReferences:\nhttps://hadoopecosystemtable.github.io\n\nhttps://www.cloudera.com/products/open-source/apache-hadoop.html\n\nhttp://statrgy.com/2015/05/07/hbase-parquet-or-avro/\n\nhttps://www.bloorresearch.com/2012/01/cassandra-hadoop/\nhttps://wiki.apache.org/hadoop/HowToSetupYourDevelopmentEnvironment\n\n\n\n', 'topics': ['security', 'azure', 'gcp', 'healthcare', 'aws', 'cloud'], 'expertise': ['management', 'management']}, {'title': 'Contactless /Mobile Payments – What is happening? An Opinion', 'content': "\n\nContactless /Mobile Payments – What is happening? An Opinion\n\n\n\n\nContactless /Mobile Payments – What is happening? An Opinion\nCreated on 2015-02-03 00:36\nPublished on 2015-02-04 00:00\n\nApple Pay has been making news in recent times – though primarily in the US; but it has yet to be introduced elsewhere. Previously Google Wallet, another payment solution leveraged Near Field Communication (NFC) technology on mobile phones allowing ‘Tap and Pay’. Once again, the rollout has had been limited to the US. Perceived progress in the way payments could be made has created excitement. Newsprint is being rolled out, opinions abound in the Internet and there understandably is desire across the board to be ones partaking of in the benefits.\nNews about changes in the industry is exciting to me as well. I look forward to being able to pay for my coffee by activating NFC on my phone. The technology is seemingly matured - my Samsung smartphone as with probably most of your phones has an option (although unutilised) called 'NFC Payment' inbuilt. So, I have been puzzled why Google Wallet or for that matter any other smartphone based NFC payment system is as yet to be rolled out in the UK. Of course, we have had the plastic contactless payments stickers for mobiles from Barclaycard for a few years now; and mobile operators like Vodafone and EE have introduced similar options – but none of them is as yet integrated with the NFC technology built into our smartphones. So, if mobile payments are so exciting and convenient, why has there not been a bigger push to roll the technology out in greater geographies?\nA simple explanation could be an argument that Google, Apple are based in the US; which apart from being popular as a favourable place for innovation, is also a big market and so minimizes risk for the companies to prove the concept first before venturing out elsewhere. Other big countries (like China or India) may not have necessary infrastructure or institutions to support adoption; and countries like UK and others in Europe may be more suitable as a place to market the end product than prove the concept. It is also interesting that although contactless payments (in particular using the NFC technology) have been around in Japan for over a decade (see a below), it has not evolved as acceptable to the wider world. I therefore wanted to delve a little bit deeper and investigate how likely the new NFC based payments systems are to be a disruptor to traditional methods you and I use to make payments.\nIn order to help my reasoning, I considered my opinion of ‘perceived value’ customers get from the various payment channels/ system against a set of randomly chosen (but relevant) common sense ical parameters - see 'value curves' at bottom of article for details.\nThe parameters evaluated include for example the cost to customers, convenience offered, ability to pay ‘on the move’, ability to make international payments, possible/preferred value of payments under the various options, ancillary benefits (like peer2peer payments, e-Invoicing for business customers) of/ from choosing one or more of the payment options. The higher a parameter scores in the vertical scale – the better value it offers to the consumers.\nOut of the analysis, a few points come to attention. I present those here for consideration – and any input that may influence the analysis is welcome.\n\nCredit or Debit Cards continue to offer higher benefits on most of the parameters being considered.\nInternet Banking provides a better value when making international payments or a payment of a slightly higher value – e.g. paying for a service provider or friends and family abroad.\nThe most traditional means of making payments – like visiting a bank branch or issuing a cheque – have as expected to be on their way out. They offer hardly any key benefits that are not available from the other means of payments. The only exception may be the ease that certain demography (e.g. the elderly) may derive from visiting a branch to manage their payments. For the majority visiting a branch for making payments is probably a lot more expensive on their time.\nThe parameters in my list that the new modes of payments (e.g. Digital Wallets, NFC payments) seem to be competing –rather quite intensely- are convenience, time saving and ability to make payments ‘on the move’. With contactless credit/ debt cards becoming the norm, ‘on the move’ payments is however not a strong parameter for the new modes of payments to compete on. The more significant parameters may be convenience – where one can get away with just carrying the mobile phone and being able to choose from which credit or debit card the payment is made! The new modes of payments however still struggle in the 'accessibility' parameter - by which I mean their acceptability by the retailers or probably financial institutions or even reliability that a transaction initiated through these means would go through smoothly . For example I personally found it impossible on two occasions to order food from Wagamama using the PayPal mobile app – finally had to abort the payment and walk into the store to pay it using the credit card!\nmobile Pay Apps - for example PingIt (which I would classify as a Digital Wallet -see b,in appendix below) offers certain additional benefits like peer2peer mobile payments without sharing personal information like bank accounts; and eInvoicing, whereby a worker may invoice his various clients with relative ease – thereby saving on manual time and effort. Unfortunately, the initial setup process - from personal experience - could be very daunting. Unlike traditional methods of payments - it is also virtually impossible to recall erroneous payments; so consumers may accept it for only small value payments. More effort is necessary to simplify the customer experience for this payment method to be generally more acceptable.\n\nI believe the industry is currently going through a flux of innovation – but a design that would be defining and act as a catalyst for growth is still not clear-cut.\nTherefore in the medium term, I foresee the following changes happening: -\n\nNumber of high street branches reduced – probably merged more along the lines of M&S Bank (services provided by HSBC) or other banks like those offered by the other retailers like Sainsbury banks catering to customers who prefer the retail experience.\nInnovations in mobile payments continuing until a dominant design evolves – something that incorporates multiple credit debit/ credit cards into its mould. It could be argued that Google Wallet/ Apple Pay may have already achieved this in the US.\nWe could see a parallel mobile payment service (as indicated by PingIt) offering benefits like simple peer2peer payments, ability to reduce commercial paperwork (e.g. eInvoicing), which could do away with some benefits of Internet Banking, as we currently know it.\n\nIn summary, I believe mobile payments continue to be promising – adoption would be driven by ease of use and accessibility (i.e.) their acceptability by merchants as well as ease of use by consumers. None of these are huge challenges – but they are important milestones that the design that would become dominant in this space has to meet!\nLast but not least, the following caught my attention this evening...\nhttp://googlecommerce.blogspot.co.uk\nIntroduced on 29th Jan 2015, this channel from Google allows one to 'email' money to friends and family in the UK even if one does not have a gmail account. Once money is 'emailed' across, the recipient can keep it in their Google Wallet or transfer across to his or her bank account. Only time will tell whether this has the potential to define how peer2peer payments are made and thereby erode the competitive advantage that PingIt has been trying to build up! As for me, I am optimistic that it may be the catalyst to usher in further growth in the industry...but lets see!\nAppendix:\na. http://www.japantimes.co.jp/news/2014/09/14/business/tech/apple-decade-behind-japans-mobile-payment-curve/\nb. http://www.datamonitorfinancial.com/barclays-pingit-extends-its-reach-to-all-uk-banks/\n\n\n\nNB: Value Curve Analysis referenced in the write up above shown as picture.\n\n\n", 'topics': ['infrastructure', 'technology', 'retail', 'innovation'], 'expertise': ['technology', 'technology', 'technology', 'technology', 'innovation', 'technology', 'innovation']}, {'title': 'Blockchain basics — Elliptic Curve Cryptography\n', 'content': '\n\nBlockchain basics — Elliptic Curve Cryptography\n\n\n\n\n\nBlockchain basics — Elliptic Curve Cryptography\n\nCreated on 2022-02-14 17:27\nPublished on 2022-02-14 17:39\nIn The Matrix, The Keymaker controls access to key information. Which includes access to the door leading to The Architect. In the video clip above, The Keymaker tries to hand over a key integral for Neo’s path as The One to Niobi. And you might be wondering what does this have to Elliptic Curve Cryptography (ECC)?!Well, let’s start with the basics of public key cryptography — which means that it is easy to secure information by keys that are based on mathematical functions that have a special property: it is easy to calculate them, but hard to find out how they were calculated. For example, the number 8,018,009 is the product of two prime numbers. It is trivial to generate it if the two prime numbers were known. However, unless one of the numbers (2003)is known, it is more difficult to derive the second (4003). Such functions are called\xa0trapdoor functions\xa0because they are difficult to reverse without prior knowledge of one piece of secret information.RSA (Rivest–Shamir–Adleman) is a public-key cryptosystem that is widely used for secure data transmission. However, in the energy intensive world of blockchain technology, it is not secure enough — RSA is still a trapdoor function and can be compromised without a sufficiently large key chain. Blockchain technology instead uses a more advanced category of mathematical functions based on arithmetic operations on an elliptic curve for which no known trapdoors are currently there.breaking a 228-bit RSA key requires less energy to than it takes to boil a teaspoon of water. Comparatively, breaking a 228-bit elliptic curve key requires enough energy to boil all the water on earth.A comparative view of RSA and ECC is nicely presented\xa0here\xa0and to visualise how much harder it is to break these encryptions, check out at your leisure the concept of Global Security from Lenstra\xa0here\xa0— note it is a slightly technical read.In the section below, we will go a bit more into the details of how private and public key pairs are generated using ECC; and how hash functions are used with the public key as input to support a number of use cases.The base relationship between a private key (k) and the public key (K) is:K = k*Gk is any integer of 32 bytes (ie 256 bits). This can be generated for example by flipping a coin 256 times and recording the outcome (1 for Head and 0 for Tail). This serves as the private key and should be kept a secret by who generates it.G is a point on the Elliptic Curve [y²= (x³+7) over a (finite) field of prime order p, where p = 2²⁵⁶- 2³²- 2⁹-2⁷-2⁶-2⁴-1]The resulting curve is visually as a collection of points (shown in the image) — it isn’t a continuous curve as the values of x and y to satisfy this equation can only be a series of discrete numbers only. The equation is solved by perfoming modulus division — the equation above can be re written as y² mod p = (x³ + 7) mod p. The operator for this in python is %Elliptic Curve PointsOne property of elliptic curves is that two points on the curve are added, the result is also another point in the elliptic curve. This means that if an integer k (acting as the private key) is multiplied with a discrete point G on the curve, the result is equal to G+G+…(k times) and it is a point again on the curve. The result K is called the public key and can be shared with others to access relevant encrypted information.Another property of the elliptic curves is that whereas K = k*G is valid; the reverse operation is not possible — ie k !=K/G. So whereas the same point G is used in all programme implementations (eg\xa0OpenSSL,\xa0libsecp256k1) and combined with a random number k to generate a user’s public key K; it is not possible for anyone to perform a division of the published public key K with the (known and common value) of G (the generator point) to derive someone’s private key. This is the most important feature which enables the use of Elliptic Curve arithmetic for public key cryoptography.In Ethereum, public keys are used to generate a number of related features. For example addresses that would seem like IBAN bank accounts (ICAP) and digital signatures. To understand how this happens, it is necessary to understand a feature called\xa0hashing. A hash function is loosely speaking a map — it maps an input data of arbitrary size to an output data of fixed size. The hashing function demonstrates a few key properties:Determinism -A given input message always produces the same outputVerifiability — Computing the hash of a message is efficientNon correlation — A small change in the input message (even the least significant bit) produces a very different hash output; so it cannot be related to original inputIrreversibility — Computing a message from its hash is not feasible — ie brute force search is not possibleCollision protection — It should not be feasible for two input messages to result in the same hash outputDue to these properties, hashing allows a variety of use cases such as data fingerprinting, message integrity, authentication, unique identifiers etc. In Ethereum the keccak-256 algorithm is used to achieve hash outputs.Now back to The KeyMaker from The Matrix. In the Ethereum world, The KeyMaker would be the wizard of keccak-256 and perform the magic and generate the various addresses — externally owned accounts (ie those that hold Ether) and smart contracts (those that contain business functionality) published on the blockchain. So, were a virtual world be create in the MetaVerse and humans’ consciously transferred over to there; The KeyMaker would be crucial for Neo to extract someone else from there. Or was it why the NSA is alleged to have placed a backdoor in the Dual_EC_DRBG ramdon number generator when the National Institute of Science and Technology attempted to standardise Keccak as part of the Federal Information Processing Standard (FIPS) 202 in 2015?! Read more about it\xa0here…I hope this article has been useful to you — please feel free to leave comments and also what you would like to know more. I will continue with further posts with The Matrix as the inspiration for learning more about blockchain.If you like this article, read about Oracles in the Decentralised Web here\n\n', 'topics': ['security', 'technology'], 'expertise': ['technology', 'technology', 'Technology']}, {'title': 'Validating the offer price of a house in the UK', 'content': '\n\nValidating the offer price of a house in the UK\n\n\n\nValidating the offer price of a house in the UK\nCreated on 2016-04-08 16:54\nPublished on 2016-04-10 17:16\n\n\xa0In this post, I will share my experience of helping a friend assess the asking price of a house for sale in the UK. The post code considered is DA3 around a town called Longfield in Kent. The house that my friend was interested in was put for sale at £550k - with the estate agent advertising that it had \'recently\' been reduced from £590k. A couple of visits later - the house was found to be well\xa0decorated and is a nice street. Although there was a point of concern (without any dis respect to anyone)\xa0about certain types of houses at further down the street, my friend really liked the house and wanted to make an offer on it. \xa0The question was how much it really was worth!\nAfter initial analysis of the Land Registry data, an offer of £490k was made. However, the vendors did not accept it as they wanted around £540k. I therefore wanted to have a second look at the data to see if it really justified the extra premium. Analysis - as shown below - now led my friend to withdraw the original offer. Based on historical and comparable data for DA3 post code and the concerned street indicate that the asking price was just too high and not sustainable from an investment return point of view.\xa0Your views are welcome on what what you think of the below.\nConstructing the basis framework to analyse\nThe UK government\'s Land Registry Linked Open Data (http://landregistry.data.gov.uk/) provides useful data sets to help in deciding on an offer price for a house.\nA report builder can be accessed from the following link (http://landregistry.data.gov.uk/app/ppd)\n\n\n\n\xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 \xa0 Fig 1. Seasonally adjusted prices in Kent\nThe graph above shows the Seasonally adjusted prices for homes in the whole of Kent. The subsequent analysis would however consider only the post code DA3 and then zoom in on a specific road in the locality of Hartley.\nAn analysis of the data dump for all detached home sales since Jan 2001 reveals the following:\xa0\n1. Homes that have seen sales were held by the previous owner on an average for 4.7 yrs - with a maximum of 14.3 yrs and a minimum of 0.1 yr\n2. The Compounded Annualised Growth Return (CAGR)* for such homes have been 9.14% on an average with a standard deviation of 21% and a median of 5%. The returns thus have had a very long tail! The maximum CAGR was 250% for a transaction on 4th Jul 2003 whilst the minimum CAGR was for -52% for a transaction on 26th May 2005. With 95% certainty**, it can be stated that CAGR on detached home transactions have varied between -32.55% and 50.82%\n3. This historical average price of houses has been £399,474 with a standard deviation of £140,057 - so a fairly distributed range. The maximum price a house in this post code could usually command with 99% confidence interval range is £759,840.\xa0\nUse of comparables to determine house prices in a given street\nHere we focus on valuing a house in a given street with the framework laid out by the analysis above. We focus by assuming that a detached house in a given street is "similar" to another thus allowing us to utilise sale of a given detached house in recent past to estimate value of one we might be interested in. Lets check this out by an example.\nA specific post code /street [Referred hereafter as WRoad\xa0to protect privacy of the vendors] is considered in all analysis below\nThe key starting assumption was that all detached houses in WRoad\xa0are "similar". This would be a normal thing to assume as those houses would have been built around the same time to a similar specification. I looked at every sale in WRoad\xa0and compared the (CAGR) return of 18 WRoad\xa0on 12th June 2015 to the sale price of the other three houses between 2002 and now. Use of CAGR allows us to smooth out the returns from a sale into a straight line - representing growth per annum necessary to reach the sale price on 12th June 2015 for 18 WRoad. We get a range of returns between 1% and 4.19%.\n\n\n\xa0\nAbove - Deriving CAGR by looking at different sales in WRoad.\xa0\nNow, assuming that the detached home that my friend was interested in sold today (10th April 2016), the price that would have been paid on it with the higher CAGR of 4.19% would have been £434,551\n\n\nAssumed Sale Price (@4.19% CAGR applied to the price of 18 WRoad - the most recent sale in the street on 3rd Nov 2006)\nAbove: Expected\xa0Sale price of a detached house on WRoad - if it happened today on 10th April 2016\nFrom here on, I varied the CAGR between 1% to 52% (the maximum CAGR for all of DA3, Longfield). The sale price could be as low as £424,005 and as high as £1,633,410. As would be expected, for the given Street of WRoad; any CAGR beyond 10% is highly unlikely without for example a major infrastructure project (e.g. the proposed Paramount Studios near Greenhithe, Kent)!\nThe likely\xa0price range for a detached house at WRoad could therefore be between £439k and £469k. A slight premium might be payable if the house has had enhancements like a loft conversion or a new conservatory built\nThus, in the end; I \xa0believe it was a wise decision to withdraw the offer. The original offer of £490k was quite on the high side anyway!\nBelow:\xa0Sensitivity Analysis of possible prices of a detached house today at WRoad (by varying CAGR to the price of 18 WRoad on 3rd Nov 2006)\n\xa0\na. Unlikely to Reduce to this price\n\xa0\n\n\n\xa0\nb. Likely Price Range\n\n\n\xa0\nc. Highly Unlikely Without Any Fundamental Change Like New Infrastructure Projects Nearby Etc\n\n\n\xa0\nReferences\n1.\xa0https://www.gov.uk/government/statistical-data-sets/house-price-index-background-tables\xa0\n* CAGR\xa0http://www.investopedia.com/terms/c/cagr.asp\xa0: Indicates a smoothened annual return for a transaction between two given dates.\n** 95% confidence interval is statistically defined as [average +/- 1.96*standard deviation]\n2. Excel sheet for reference : If anyone wants the excel sheet for reference, please let me know.\n\n\n', 'topics': ['infrastructure'], 'expertise': []}, {'title': 'Banking Payment Systems - Part 1', 'content': '\n\nBanking Payment Systems - Part 1\n\n\n\n\nBanking Payment Systems - Part 1\nCreated on 2015-08-21 14:35\nPublished on 2019-07-04 23:28\n4th July 2019: Whilst going through my Articles on LinkedIn, I noticed this one that I had drafted in March 2018 but never got across to completing and publishing. I believe the article is still relevant and so renamed it Part 1. It is built on my experience having worked at most of the modules of the payments processing value chain I describe below. I will complement it with a second part describing how innovations in payments technology have evolved in the last six years and where I believe they are headed in the next five to ten years - given the strategic drivers, market forces, new technology, and the evolving regulatory framework. Any opinions I make in this process are personal or reflective of the industry consensus/trends and not of my past employers. I welcome your views or comments.Start of the article.Lets us start by looking at a schematic of how a payment historically has been processed within a banking institution. Customers can make payments through a variety of channels. It then goes through a series of validation and processing steps before it is either authorised or rejected.Fig1 - Payment Processing SchematicFig 2 - Key processing checks when authorising paymentsFig 3 - Different stages in the payment processing value chainWhy do payments sometimes get delayed?If as shown above, payment processing could be straightforward, one may wonder why payments processing does occasionally get delayed. In order to understand that, please refer to the diagram below. Failure points can be on the remitter side or the beneficiary side as well as in one (or more correspondent banks) -see Fig 4- that might exist in the payment chain. Apply the complexity of multiple jurisdictions, time zones and myriad of internal systems and quite soon you get the complexity associated with reconciliation.Fig 4 - Interbank payment flowDue to the complexity of the payments processing value chain, banks have had to build elaborate reconciliation systems. Going back a few years, it would involve a end of day triangular between the payment system, accounting systems, and the reconciliation system - see Fig 5 below.Fig 5: End of day reconciliationThe foundational elements of payments processing that we discussed above have been the prevalent norm for only a few years ago. Of course, recent developments have both disrupted and created opportunities in the processing value chain.I will follow up the next part of this article with information on Payment Schemes (what they are), how various financial organisations communicate with one another, the regulatory mechanisms that govern those interactions. We will also review the incumbent payment system providers including the ongoing Instant/ Real Payment initiatives and what future landscape when considering the technology developments from blockchains, IoT, and smart cities may mean for banking payment systems.\n\n', 'topics': ['technology'], 'expertise': ['technology', 'technology', 'technology']}, {'title': 'Driving Agility: Maximising Value', 'content': "\n\nDriving Agility: Maximising Value\n\n\n\n\nDriving Agility: Maximising Value\nCreated on 2019-07-03 23:46\nPublished on 2019-07-03 23:54\nI was recently asked how I would maximise value generation in a multi-team digital product development environment. When responding to the question, I drew upon my professional training and experiences to draw up a sketch of the context and meaning of (a) what is ‘value’ (b) what some of the key principles and enablers of value creation or value loss are; so that (c) those responsible for value generation remain aware of and can measure the outcomes and benefits. My intention is to articulate the same here and seek your opinions and draw upon your experiences of how you may relate to it.Before we go into the specifics, I would like to start by stating that the principles that I attest to are: responsibility, empowerment, transparency, and accountability. For with responsibility comes accountability, but for responsibility to be credible requires (feelings of) empowerment — without which the stage isn’t really set up for success. Responsibility, as well articulated in the accompanying short video, promotes empathy, and a drive to appreciate the bigger picture so that one feels and is empowered to make effective and timely decisions to do a brilliant job than seeing it as a series of tasks.“The price of greatness is responsibility.” ~ Winston Churchill.Next, let us consider for a moment what ‘Value’ is and some example of value creation and value destruction. ‘Value’ can be subjective — as Marty Roberts sang “One Man’s Trash Is Another Man’s Treasure”. For ‘Value’ to be objective therefore, the context is important. Let's consider an enterprise that sees a very attractive opportunity with a government regulator dangling the carrot to upgrade the national infrastructure and technology to improve inclusivity, accessibility, and choice for the population. If so, how does the firm align its corporate strategy (the why, the what and the how) to march ahead in support of the business case, stay ahead of the competition, design factors to differentiate its product proposition to appeal to the most appropriate market segments. All of these thoughts would crystalise into a chain of activities to generate a set of coherent capabilities that could be leveraged to drive suitable outcomes in support of the business case. In management parlance, the chain of sustainable activities is variously called Value Chain or Value Streams and quite simply are a series of logical steps to achieve an objective. In the example, we are discussing the value chains for the relevant journeys of acquiring new customers and upselling to existing customers could look as shown below.a. Acquire New Customers:Advertise Products > Generate Leads > Engage With Prospects > Acquire New Customers > Positive Early Onboarding UX (setup, interaction)> Provide Ongoing Simple UX (eg accessibility, billing) > Provide Upgrade / Downgrade Options > Drive Loyaltyb. Leverage Existing Customer:Generate Awareness > Provide Upsell Options (eg Promotions) > Provide Improved UX (e.g. accessibility, home move, price transparency) > Drive Ongoing LoyaltyOur objective should be to align our ducks to drive responsibility and empowerment so that while working within the constraints of budget, and schedule; the essential capabilities could be scoped in and to generate the desired outcomes and benefits. In more traditional ‘ways of working’, this is done through a series of projects whose focus is to generate a series of outputs (eg a new channel or an improved billing system) in the expectation that they would be coherent. Unfortunately, the ‘project specific’ thinking results in a temporary organisation for a specific set of changes — inadvertently, leading to siloed and disconnected ways of working across an enterprise. Significant management effort is necessary to broker relationships between teams resulting in inefficiencies and creating distance between the business objectives and those who are meant to be responsible for enabling the business. Given that projects are essentially temporary, there aren’t always opportunities to build long term working relationships amongst team members and the behaviour, in general, is reactive rather than participatory and proactive.In contrast, a ‘product based’ thinking/ approach funds a set of teams on an ongoing basis to incrementally build and improve capabilities to support the business objectives — encouraging direct, improved collaboration and continuity between business and technology teams as well as joint ownership, responsibility and empowerment to determine and negotiate sensible changes to support the value streams. In our fictitious example, assume that the customer preferences are unlikely to be as yet well understood for the new proposition, and so rapid feedback is necessary for success. With possibilities of substitutes becoming viable in the medium term, creativity is necessary to generate effective value propositions. In the context of the developing circumstances, it is, therefore, more valuable to ‘fail faster’ and recover from it. Under such a circumstance, a ‘product based’ approach is more valuable than a mere ‘project-based’ approach.Do refer to this\xa0well-written article in martinfowler.com\xa0too on the key differences between project and product based approaches.Referring back to the value streams for our sample example attempting to leverage the industry drivers, an effective approach would be to organise itself vertically into a set of relevant “tribes” or “journeys” aligned to the value generating streams— assume two, one with the mandate to acquire new customers and another to upsell to the existing customers — as the firm builds and rolls out its new offerings. The tribes could organise themselves as squads reflecting the agreed value streams within the journey definition and work closely with empowered product owners from the business to generate products that can dovetail with capabilities; either existing or new, to allow benefits to be generated and measured in much shorter cycles. As the squads are aligned to the steps in the value streams and are cross-functional, with deep domain and technical expertise, they are able to adapt to changes in relevant drivers (for example market or regulatory) to correct course and continue generating the most optimal outputs. The “tribes” are masters of their own destiny and demonstrate\xa0tribal behaviours— controlling their own budget and self-organising themselves instead of being dependent on external factors like their share of the sum of “estimated” budget amounts of a number of projects, that by their own nature are temporary and may lose strategic relevance for the organisation. If an organisation still likes to drive change through projects, the projects can pitch the changes to the tribe or journey leads; with the decisions being the outcome of a negotiation between the portfolio/ programme function and the tribe journey leads.As final thoughts, I would also like to share the following insights from Mckinsey & Company about essential changes to operating models to achieve true agility to drive value maximisation.Use case reference: ING’s Agile TransformationING's agile transformationEstablished businesses around the world and across a range of sectors are striving to emulate the speed, dynamism, and…www.mckinsey.com\n\n", 'topics': ['infrastructure', 'technology', 'agile', 'strategy'], 'expertise': ['technology', 'strategy', 'management', 'management', 'technology']}, {'title': 'Chatbots, recruitment and GDPR', 'content': '\n\nChatbots, recruitment and GDPR\n\n\n\n\nChatbots, recruitment and GDPR\nCreated on 2018-03-23 17:47\nPublished on 2018-03-23 18:05\n\nRecruitment is driven by data. To be effective, and successful, agencies and their individual agents must build an extensive network of contacts and potential candidates they can call on when a suitable opportunity arises. If they can’t find a suitable candidate in their existing network, they need to be able to quickly build new connections and find the perfect person for the role.\nThat got me thinking about how this, in practice, might be affected by GDPR. In a world where you must get consent in order to contact individuals, and follow strict guidelines around the processing of data, how does a recruitment agent’s years of network-building and contact list building stand up?\nAs a passionate advocate of innovative technologies, I’ve recently produced a how-to video demonstrating how chatbots can be used by recruitment agencies to facilitate interaction with candidates and contacts as well as maintain data and an auditable trail of processing and consent.\nUsing this technology, recruitment agencies, who are busy trying to build relationships and meet targets, will be able to continue to rely on their network whilst remaining compliant. The individual can manage and check their communication preferences via the bot, and should they need to amend data or ask for it to be removed, it can take care of that too.\nChatbots have the capability to not just help a range of organisations be compliant with GDPR, but also to transform and drive efficiency and productivity. If you would like to find out more about how chatbot technology could transform your organisation, as well as help you navigate a complex legislative landscape, I would love to chat.\xa0\nWatch my how-to video here:\nhttps://www.youtube.com/watch?v=L9ldTMLjauA\n\n\n\n', 'topics': ['technology'], 'expertise': ['technology', 'technology']}, {'title': 'NTT Data’s Open Innovation Event (London 22.02.2018)', 'content': '\n\nNTT Data’s Open Innovation Event (London 22.02.2018)\n\n\n\n\nNTT Data’s Open Innovation Event (London 22.02.2018)\nCreated on 2018-02-22 22:30\nPublished on 2018-02-22 23:11\n\nToday I attended NTT Data’s Open Innovation event at Level 39 in Canary Wharf. It brought together a mix of start ups promoting value propositions in the field of banking, media, and internet security. As the event name would suggest, machine learning, AI and IoT were key facilitators in the innovations.\xa0\nIn this post, I wanted to brief on these start ups. I also summarise some of the themes covered in the panel and keynote discussions that following the pitch sessions.\nParticipants\nOn the media and advertising space, there were CrowdEmotion and DataSine.\xa0\nCrowdEmotion, who inspect facial muscle and eye movements to measure brand engagement providing near real time feedback to a host of media houses.\xa0\nThen there was DataSine, who utilise psychometric profiles of target segments to personalise advertising - thereby ushering in meaningful customer interactions.\nOn reducing cyber risks were Callsign and Onfido.\nCallsign use multiple features such as Location, Data content, time zone, ‘behaviour’ to secure identity of devices communicating over the cyber space. I see a huge market potential for them in the evolving IoT space.\nOn the other hand, Onfido with its multi dimensional audio visual identify management solution has market in the ‘know your customer’ segment.\nIn addition,\xa0Cambridge Quantum Computing stood out in my opinion for the depth of their offering. They aim to make quantum computing and security a reality for humanity and accessible to non specialists - with collaborations that span multiple research and commercial organisations.\nOn banking, the participants included Worapay, Akoni, Aixigo and MarketIQ.\nWorapay aim to allow high street banks to compete with the challengers disrupting the value chain with their sleek POS solutions replacing the traditional credit and debit card machines. They also aim to reduce lost opportunity costs for high street institutions such as coffee shops with their ‘payment without queues ‘ solution.\nAkoni aims to allow businesses choice, accessibility and control to options for getting better returns on their deposits across the participating financial services organisations.\nAixigo claim to provide instant portfolio value calculation in the wealth management space.\xa0\nMarketIQ suggests its focus to be allowing incumbent financial institutions utilise data to provide value added services and insights to their business banking customers instead of acquiring customers by commoditised promotion of their traditional services.\nOther discussion points\nThe keynote discussion focussed on attempts to synergies fintechand regtech innovations - fin tech in the context of the evolving new technology interactions between customers, businesses and financial institutions; and regtech in context of utilising technology to allow financial institutions comply with the increasing regulatory framework (eg bcbs239, solvency 2 etc). The increasing cost of regulation was acknowledged as both a problem and opportunity to innovate.\xa0An example was stated of some banks maintaining teams in triplicate for regulatory reporting because of idiosyncratic requirements in interchange formats and associated skills. In that context, the panel focussed on promise of xBRL (a rich XML format) as an opportunity to construct a common schema-less regulatory reporting data interchange.\xa0\nThe event ended with certificates of appreciation to all participants. Callsign was chose as the regional finalist to participate in the global round in Tokyo in March 2018.\nFinally I would like to acknowledge the lively crowd I engaged with this afternoon.\n\n\n', 'topics': ['security', 'ai', 'machine learning', 'technology', 'innovation'], 'expertise': ['Innovation', 'Innovation', 'Innovation', 'management', 'management', 'technology', 'technology']}, {'title': 'Basics of Big Data Compute and Storage', 'content': '\n\nBasics of Big Data Compute and Storage\n\n\n\nBasics of Big Data Compute and Storage\nCreated on 2018-01-05 17:30\nPublished on 2018-01-05 20:13\n\nBig Data Technologies have seen an evolution over the years. Over a few posts in comings days, I will provide simplified view of what the different technologies do and how they form the ecosystem. This is based on my work with these technologies over the last year or so. If any comments, please do share. \nTo start with I provide a high level summary of the base compute and storage technologies associated with Big Data. They are the building blocks of what we will see is a very fast evolution into capability today from the likes of Google, Amazon and Azure to stand up on the fly big data platforms for anyone wanting to process vast data sets. I will share my views on that journey over forthcoming posts.\nBase Compute Technologies:\nMapReduce - There are two parts : a \'mapping\' process, where all input data are mapped to specific key values. The Reduce process then solves the problem statement. \n\nFor example if there were a sentence in the input file and the sentence said "Hello I am well"; then the mapping process will result in the following key value pairs\n\nH,1; e:1; l:l; l:1; o:1; I:1; a:1; m:1; w:1; e:1; l:1; l:1 \n\n\nThose key value pairs are then sorted (as part of the same \'map\'ping exercise)\n\na:1; e:1; e:1; H:1; I:1; l:1; l:1; l:1; l:1; m:1; o:1; w:1\n\n\n\nIf the problem statement was to find the number of occurrence of each character in the input statement, the reduce process will add up the values associated with each of the keys above and give us\n\na:1; e:2; H: 1; I: 1; l:4; m:1; o:1; w1\n\nSo we know that l occurred a total of 4 times in the input statement. \nThat in a gist is what MapReduce does. Of course, this is a trivial example - the point was to explain the concept in a simple manner. MapReduce is used for activities as complex as Fraud Detection, Customer 360, Movie Recommendations etc. \nThe real power of MapReduce comes from the technological ability to distribute the key value operations over many, many commodity computers. All this compute power is managed as a cluster - there are is a \'named node\' in a cluster which keep track of the \'data nodes\' where the actual computations happens. Storage disks are usually attached to the named and data nodes - either in memory or as attached volumes. The attached storage volumes required special formatting as a distributed file system (HDFS on most Hadoop distributions). MapReduce use HDFS for all processing results.The computations usually have multiple data nodes doing exactly the same computation built in - so that any specific \'data node\' failure doesn\'t affect final outcome - which is a glorified patching up of individual chunks of compute outcome. \nMapReduce as a technology was suitable for large scale batch processing - however, it wasn\'t suitable as is to process streaming data due to always reading from and writing to attached storage (disks). Ability to process streaming data was crucial in an age of IoT with sensor data and machine learning and AI requirements.\nSpark - Spark provides better processing performance over MapReduce. It does so through in-memory computation. Although Spark utilises MapReduce under the hood, it abstracted a lot of MapReduce lines of code into higher level functions. This brought in processing efficiencies making it suitable for processing streaming data. Spark core engine also opened the door for creation of libraries allowing for Machine Learning, Graph Computation, functional programming in R, SQL-like access to data etc.\nBase Storage Technologies:\nHDFS Vs Object Store and the gradual coming together of the two\nHDFS is a distributed file system accessible to all compute nodes of an hadoop cluster. That was the base architecture designed for Hadoop. Libraries have then been developed to allow Hadoop to read from external data sources. \nObject Store contains data in a key:value pair and does away with limitations associated with file systems - limitations such as network latency induced limited scalability, data storage within file system hierarchy induced accessibility limitations etc. ObjectStores allows data to be stored in a \'flat\' structure and a simple ID allows speedy retrieval of the required data set. As such it was great for Web 2.0 companies trying to build out high performance applications, data analytics etc. The required information could be easily extracted from an ObjectStore and pushed into the Hadoop cluster for onward processing. Base data like tweets, facebook posts, google pictures would just be dumped into an ObjectStore for appropriate utilisation later on.\nAs the technologies evolved, it became natural that it could be possible to completely allow HDFS and ObjectStores to work in greater sync. One driver is to reduce overhead costs associated with traditional Hadoop, where compute power and storage scale in sync. If we want more storage, we have to add more cpu as well. With ObjectStore architecture however, that limitation isn\'t there. \nHowever, for ObjectStores to work natively with Hadoop, the ObjectStore nodes would need to provide HDFS compatible storage and the CPU power to run the MapReduce process as well. The efforts to make ObjectStore and Hadoop compatible are also driven by fears of data byte losses as the aggregate volume of data in hadoop (ie HDFS) clusters exponentially grow. With future data growth in mind, the current default 3x replication usually configured in HDFS is at risk of being insufficient to avoid significant risk of losing a byte. If Hadoop can ben integrated to work with ObjectStore, then the risk is avoided because it is then very easy to safe store the metadata from which any lost data can be reconstructed by any node in the cluster.\nIn coming series, we will go over the other components of the Hadoop ecosystem. I will also share different architectures of Hadoop and and how Hadoop components enhances various capabilities such as data ingestion, data lineage, data life cycle management. We will also look at tools in the ecosystem that provide data wrangling and data visualisation.\nThank you for your time.\nReferences: http://searchstorage.techtarget.com/feature/How-an-object-store-differs-from-file-and-block-storage\nhttps://www.datanami.com/2015/06/23/data-lake-showdown-object-store-or-hdfs/\nhttps://it.toolbox.com/blogs/maryannrichardson/3-reasons-to-replace-hdfs-with-object-storage-and-1-reason-not-to-103117\n\n\n\n', 'topics': ['azure', 'ai', 'architecture', 'machine learning', 'data analytics', 'technology'], 'expertise': ['technology', 'architecture', 'architecture', 'management']}, {'title': 'Cloud Platforms for Big Data', 'content': "\n\nCloud Platforms for Big Data\n\n\n\n\nCloud Platforms for Big Data\nCreated on 2018-03-01 18:09\nPublished on 2018-03-02 11:32\n\nThis is third of the series of articles I decided to write illustrating the evolution of the Big Data processing frameworks. \nWhereas the first article covered the basics of big data compute and storage, the second article covered the considerations for contracting a big data platform. In this article, I will summarise the three primary cloud providers and the toolkit they have made available to us.\nOf the three key cloud providers, my favourite is GCP for its simplicity of adoption. I had taken a course on the fundamentals of the core infrastructure arond google cloud platform in Coursera and was quite impressed with the simplicity and breadth of tutorials which an user could onboard the platform.\n\n\n\nAWS is however the incumbent and leader in cloud computing with 62% of the market share. Azure's share is 20% and GCP is at 12%. Whereas all three provide similar services, AI, IoT are increasing seen to be the differentiator in the future and the market expected to continue growing fast. It would in this context that simplicity to adopt, and operate at enterprise scale. Organisations are also increasingly considering federated cloud services to use the best of breed from all providers- so it isn't always competition between the primary platform; instead a mix of competition and collaboration. It is in everyone's interest to make their services plug and playable - so they can coexist to enhance's customers' value chains and business propositions.\nHere is a nice article driving home this point:\nhttps://www.cmswire.com/information-management/aws-vs-google-vs-microsoft-who-will-win-the-cloud-and-does-it-matter/\n With some much said, I include below a few info graphics describing the various services from GCP, AWS and Azure.\nGCP\n\n\n\nhttps://cloud.google.com/terms/services\nRead here about what each of these Google services provide.\nAWS\n\n\n\n\nRead here about what each of these AWS services provide.\n\nAzure\n\n\n\n\nRead here to know more about the Azure service offerings for big data\nOther references: \nhttps://www.slideshare.net/HadoopSummit/big-data-in-azure-65742328\n\n\n", 'topics': ['azure', 'ai', 'gcp', 'aws', 'cloud', 'infrastructure'], 'expertise': ['management']}]}}, 'medium': {'articles': [{'title': 'Could AR rejuvenate the high street?', 'content': 'Could AR rejuvenate the high street? Could AR rejuvenate the high street? In recent years, we have seen a marked change in consumer preference for shopping online at the expense of the high street retailers. The… Could AR rejuvenate the high\xa0street? In recent years, we have seen a marked change in consumer preference for shopping online at the expense of the high street retailers. The likes of Amazon and Ocado have taken away the need to visit the local supermarket to order groceries or get the latest gadget. A few months ago, I even ordered my pair of specs and sunglasses online\u200a—\u200aI was amazed at the options available. But then it also triggered a thought about how technology might have come full circle and redefine what we expect and how we experience the high street. This is where I start with my story: One weekend, I had gone to a Vision Express store and after almost an hour covering an eye test and then exploring various models requested the same sunglass frame as I had ordered with the retailer a couple of years ago. The salesperson happily took the order and I paid for it too. I also ordered a second frame for daily use. The expected delivery time was 3–4 weeks. After a few weeks, I chanced upon a voicemail from the store that they no longer traded the sunglass frame and so could I come back and order a replacement. I was slightly disappointed but did return after work one evening. The salesperson showed me other options but did not seem to acknowledge the inability to honour the original order or appreciate my extra effort in coming back to the store. The new options weren’t also as valuable as the previous one. So, out of frustration, I decided to cancel the entire order (not an insignificant amount) and explore other options\u200a—\u200aincluding what I had vaguely heard about the ability to order glasses online! That turned out to be insightful\u200a—\u200aboth in terms of how the retail stores are increasingly at a disadvantage but also how technology may be a panacea. Trending AR VR Articles: 1. Oculus Go, the Nintendo Switch of\xa0VR 2. Ready Player One\xa0: How Close Are\xa0We? 3. Five Augmented Reality Uses That Solve Real-Life Problems 4. Virtual Reality Headsets: What are the Options? Which is Right For\xa0You? On Customer Experience and the desirability of the high street High Street shopping offers a social and community experience and so is valuable in its own right. That social and community feeling alongside an expectation of a pleasant experience are factors that in my opinion, motivate us to go to brick and mortar shops. However, larger overheads, increasing cost pressures, and lower profitability margins are putting brick and mortar retailers at risk. Online shopping, especially for goods we know well of, is more convenient and time optimal. I was however pleasantly surprised to find that the experience of ordering my pair of specs online\u200a—\u200aassessing, selecting the relevant items and subsequently paying for it as well as getting delivery was all reasonably straightforward. I had a clear view of how to find out the right frame size, what items were in stock, where the manufacturing happens and how it is delivered to my home address. Now if this becomes the norm, then optical retail shops may be another casualty of the online spree\xa0… Courtesy: The Guardian Newspaper, 9 Nov\xa02018 There is, therefore, an urgent need to reinvent retail optician shops. Currently, the eye test is a valuable service that retail optician shops provide\u200a—\u200aexcellent equipment, professional staff, and personal service. However, in my opinion, the primary revenues in optical shops come from the sale of the optical inventory\u200a—\u200aframes, lenses, et al. However, as evident through my example, online channels may increasingly overshadow the retail channel in selection and ordering of optical wares. That would then threaten the retail shops\u200a—\u200awho may have to limit their services to bare essentials like eye testing. Innovative and developing technology like AR (Augmented Reality) however may provide a new lease to high street shopping experiences\u200a—\u200aincluding our optical shops. Consider for example your better half or your friend spending an afternoon in the high street. She notices a pair of sunglasses, which she may want to surprise you with. You had only recently had your eye test and shared the results with your friend. Your friend goes into your Facebook or Instagram profile, sources a picture or a video of you and uploads to the sleek equipment in the shop. The equipment projects a holographic image of your face and allows your friend to select different frames and shades and decide upon the best fit. Then it is about confirming the order and organising a delivery and voila you get a new pair of trendy sunglasses as a gift! Wouldn’t that be great? Considering today, it is not really feasible for anyone else apart from yourself to order a pair of glasses? For someone like me, it would be wonderful\u200a—\u200aconsidering I tend to get a new pair only very far between! For the brick and mortar optical shops, this will generate more footfall and sales. It is also beneficial for the high street as both online and offline shopping experiences then complement each other than cost one another. Applications of AR have already made their mark visible. We are seeing AR or its cousin VR(Virtual Reality) bring new shopping experiences in clothing, gaming as shown below. AR in Shopping (Magic)\xa0Mirrors Augmented In-store shopping experiences Indicative of the trends and examples discussed above, Gartner predicts that Augmented Reality will in the next 5–10 years reach the ‘Plateau of Productivity’ between 2023 and 2028 and in the interim the technology will climb the ‘Slope of Enlightenment’. So, it would exciting times ahead for AR technology and I believe it has strong potential to reshape our high streets for the better. And Finally\xa0… An image of my experience shopping the first pair of specs online\xa0:-) Selecting and assessing a pair of Specs virtually at specsavers.couk Don’t forget to give us your 👏\xa0! By Mondweep Chakravorty on March 15, 2019 . Canonical link Exported from Medium on January 13, 2025.', 'date': 'March 15, 2019'}, {'title': 'New Ways of Working Transformation', 'content': "New Ways of Working Transformation New Ways of Working Transformation Why talk about “New Ways of Working” in the first place? New Ways of\xa0Working Why talk about “New Ways of Working” in the first place? It is best answered by ‘Murmuration’, see the video below…. A collection of starlings is called a Murmuration. “Complicated behaviour can be caused by relatively simple individual patterns”. W. Brian Arthur, Pioneer in the science of complexity theory The video shows thousands of starlings doing the same thing : staying as close as possible to those around them, resulting in complex figures . “Individual actors react to economic situations created by others. This changes the situation, causing other actors to change their strategies again”. This creates a world in which individuals, banks, companies, and investors are always adapting to the situation around them. It’s a world where “Adaptive strategies and actions are constantly being ‘tested’ for survival.” In order to survive, individuals and organizations need to keep evolving and adapting their strategies to their changing (economical) context. Organizations incapable of changing quickly enough will cease to exist. That is why in the world of rapidly evolving business models, adapting to ‘New Ways of Working’ has become a matter of survival for organisations of all shapes and sizes. The team at Corporate Rebels visited 150 pioneers, rebels and revolutionaries —\u200amade up of organizations, entrepreneurs, academics and business leaders (see map below) who change the status quo of frustrating workplaces. Insights from meeting and speaking with these leaders resulted in 8 trends and practices that were crucial in transforming ways of working in their various organisations covering Government, IT, Services, and Manufacturers. Those trends prominently highlight a transition of patterns from what is considered traditional ways of working into what is increasingly being called ‘New Ways of Working’. From ‘Profit’ to ‘Purpose & values’ From ‘Hierarchical pyramid’ to ‘Network of teams’ From ‘Directive leadership’ to ‘Supportive leadership’ From ‘Plan & predict’ to ‘Experiment & adapt’ From ‘Rules & control’ to ‘Freedom &trust’ From ‘Centralised authority’ to ‘Distributed decision making’ From ‘Secrecy’ to ‘Radical transparency’ From ‘Job descriptions’ to ‘Talents & mastery’ You can read about the trends in detail here along with reference case studies. The trends listed above echo the generally promoted principles of agility that most of us have possibly come across\u200a—\u200afocus within an organisation on a culture value generation, avoiding waste, promoting trust and transparency. The music company Spotify promotes, for example, the following culture within its Engineering practices. What has become known as the Spotify Model has been the subject of research and discourse in reputed research journals for example here about balancing employee autonomy, accountability and here about agility at scale. Spotify Engineering Culture Part\xa01 Spotify Engineering Culture\u200a—\u200apart\xa02 To summarise, the Spotify Model promotes the following patterns: Agile Principles over Scrum Practices Servant Leadership over being a Master. A leader’s job is to communicate what problems need to be solved and why. The squad’s job is to communicate with each other and find the best solution. Different squads follow a self-serve model and enable each other. Be autonomous but don’t sub optimise -alignment enables autonomy Follow a community approach over organisational structure Cross-pollinate practices over standardisation Enable an internal open-source model Small but frequent releases. Decouple releases. Utilise release trains with feature toggles to reduce technical debt and optimise feature releases. Focus on motivation. Value people. Trust is more important than Control. Agility at Scale requires Trust at Scale. Practice Continuous Improvement. An experiment and fail friendly environment, where failure recovery is valued over failure avoidance. Limit the ‘blast radius’ by a decoupled architecture and gradual release of new features. Minimise the need for big projects. A healthy culture fixes broken processes\u200a—\u200avalue delivery is more important than plan fulfillment. Adopt a waste repellent culture. Value innovation over predictability\u200a—\u200aprovide for ‘hack time’ and organise ‘hack week(s)’ to make “cool things” real. People are natural innovators. If we distill the patterns, we would see them correlate to the 8 trends that we saw earlier. engineering culture (part 1) Here's part 1 of short animated video describing our engineering culture (here's part 2). This is a journey in… labs.spotify.com Spotify engineering culture (part 2) Here's part 2 of the animated video describing our engineering culture. Check out part 1 first if you haven't already… labs.spotify.com Use Case: Agile Transformation at ING Bank ( https://agilebusinessmanifesto.com/agilebusiness/agile-transformation-at-ing/ ) To sign it off here is a nice article that reinforces some of the principles and patterns discussed earlier with examples from the industry. Digital transformation is not about technology\u200a—\u200ait is about understanding the underlying business’ strategy, trusting and leveraging the knowledge of the insiders, engaging with customers, optimising existing processes, and overall bringing in a start-up culture. By Mondweep Chakravorty on November 17, 2019 . Canonical link Exported from Medium on January 13, 2025.", 'date': 'November 17, 2019'}, {'title': '100GB of monthly mobile data allowance\u200a—\u200awhy not arbitrage?', 'content': '100GB of monthly mobile data allowance\u200a—\u200awhy not arbitrage? 100GB of monthly mobile data allowance\u200a—\u200awhy not arbitrage? A few months ago, I upgraded to a new phone and a 100GB data package at a very attractive price. My monthly usage has historically been… 100GB of monthly mobile data allowance\u200a—\u200awhy not arbitrage? A few months ago, I upgraded to a new phone and a 100GB data package at a very attractive price. My monthly usage has historically been perhaps around the 5GB mark at the most. So, this for me has been a luxury\u200a—\u200aI can listen to songs, stream videos endlessly without having to worry about going over! However, I thought how nice it would be if I could sell the 95GB of my monthly data allowance to others who may require that capacity and benefit from the arbitrage? Surely, there is always demand from tourists who are on holidays or others who may temporarily be around my location but whose mobile provider coverage may not be as good as my provider’s. There may even be demand from small businesses who may be able to pool together data packages from multiple retail users like I than rely entirely on business data plans from the ISPs. In the last few years, distributed ledger technology has seen reasonable practical usages*. Blockchains (as the technology is more commonly known) allows ‘smart contracts’ to automate the settlement process. This inspires me to think of over the top innovations to trade excess data with market segments and generate value for not just the buyer and seller but also for the telecoms providers. The concept is time tested in other areas of telecoms\u200a—\u200aconsider discount mobile service providers like Lycamobile whose business started by targetting specific (eg with calling plans for specific countries) demographics and offering data and voice products that arbitraged the wholesale and retail price points of unutilised bandwidth from the major telephony service providers like BT, Vodafone etc. The diagram below indicates the salient points about the hypothetical product and the demand-supply relationships that could sustain it. I also include a conceptual architecture diagram that could form the foundations of this proposition. I welcome your views on this. Below is a conceptual view of how a distributed ledger for trading excess data may look like and how it could generate value for the buyer, seller as well as the telecoms providers: Win-Win-Win for all parties…. *To know more about tangible blockchain initiatives in the Telecoms industry, read more here -> https://www.ledgerinsights.com/category/industries/technology-media-telecoms/page/6/ By Mondweep Chakravorty on March 15, 2019 . Canonical link Exported from Medium on January 13, 2025.', 'date': 'March 15, 2019'}, {'title': 'Oracles in Decentralised Web', 'content': 'Oracles in Decentralised Web Oracles in Decentralised Web The Oracle is a major supporting character in The Matrix franchise. She is a program with such insight into human psychology that she has… Oracles in Decentralised Web The Oracle is a major supporting character in The Matrix franchise. She is a program with such insight into human psychology that she has clairvoyant abilities. In one of the encounters with Neo, it is she who directs Neo to find the entryway to the Source by finding the Keymaker, a program that can open hidden and locked backdoors within the Matrix. The Matrix franchise a fictional series will help us relate to a number of concepts in what is being called Web 3 —\u200athe Decentralised Web. Let us talk about Oracles today. Oracles are systems that provide external data sources to execute smart contracts. They live and are managed off chain\u200a—\u200aand their data enabled distributed apps (dApps) perform actions and make decisions. To provide a loose analogy, if the blockchain were the computer, we could say the Oracles provide internet access to it. There exist Oracles that handle a number of use cases today\u200a—\u200aa small selection is captured here. Bonds are used to raise capital. Oracles can provide various verifiable data such as interest rates, debt scores, fiat payments, and more to settle bonds on an ongoing basis. Energy sector : In order to improve efficiencies in energy delivery, energy industry data Oracles can feed consumption rates into a smart contract to trigger over-consumption penalties, levy CO2 taxes, and provide current energy prices to fairly generate electricity bills and allow payments in different currencies. Smart contracts can take readings from smart meters to monetise someone’s output, track consumption, and facilitate payouts between the two. Social media and identity management oracles can tie ownership of domains onto the blockchain. This improves user experience, as the oracles help transform the long hexadecimal crytographic addresses into human-readable names like “chad.crypto”. A number of new Oracle ideas are currently at different stages of development across a number of sectors and there are a number of platforms (e.g Polygon , Chainlink )that provide the frameworks and capabilities to build more. A few in a nascent stage can be referenced here . Now, back to Neo and The Matrix! Whereas Oracles certainly power and steer the dApps to execute transactions and contracts in the blockchain, there is another aspect that calls for some thought. I read today that the Indian government plans to tax any digital asset transactions. Many countries are also going to issue crypto versions of their currencies. Those of us wondering what the shape of a regulatory framework may look like and provided we see regulatory convergence ever in the crypto space, may perhaps see Oracles hypothetically become a mechanism to enforce some level of control and compliance among the dApps! Your thoughts and contribution to the discussion is welcome. By Mondweep Chakravorty on February 2, 2022 . Canonical link Exported from Medium on January 13, 2025.', 'date': 'February 2, 2022'}, {'title': 'Causality\u200a—\u200aUnderstanding Counterfactuals, Attribution and the Relevance to AI development and…', 'content': 'Causality\u200a—\u200aUnderstanding Counterfactuals, Attribution and the Relevance to AI development and… Causality\u200a—\u200aUnderstanding Counterfactuals, Attribution and the Relevance to AI development and… In the quest for understanding our complex world, we often find ourselves asking, “What caused this?” or “What would happen if…?”. These… Causality\u200a—\u200aUnderstanding Counterfactuals, Attribution and the Relevance to AI development and explainability Explaining AI In understanding our complex world, we often find ourselves asking, “What caused this?” or “What would happen if…?”. These questions, invoking the idea of cause and effect, lead us into the domain of causality. This blog post will explore the science of causality, its relevance in addressing counterfactual scenarios and in solving attribution problems. What is Causality? Essentially, causality is about linking cause and effect\u200a—\u200adetermining how and why certain events occur due to specific factors. This seemingly simple principle underpins numerous scientific fields, from physics and chemistry to economics and social sciences. Its language is used to understand and predict phenomena, infer conclusions, and make decisions. Causality transcends mere correlation. It tackles the ‘why’ behind the ‘what’, lending insight into not just what events occur together, but which event leads to another. The key to understanding causality is to remember this oft-repeated phrase: “Correlation does not imply causation”. This means that just because two variables move together, it does not imply that one causes the other. Causality and Counterfactuals: The field of causality plays a fundamental role in understanding counterfactual scenarios\u200a—\u200ahypothetical situations contrasting with reality, asking “What if things were different?”. Counterfactuals help in assessing the consequences of alternate actions, providing key insights into fields like policy-making, law, economics, and artificial intelligence. Understanding counterfactuals requires the ability to model causal relationships accurately. For example, if you’re assessing the impact of a public health intervention, you’d need to compare the current situation (where the intervention was implemented) against a counterfactual scenario (where the intervention was not implemented). This comparison gives you the causal effect of the intervention on public health. Causality in Attribution Problems : Attribution problems deal with allocating the effects observed in a system to their respective causes. For instance, in digital marketing, there may be a need to understand how much each advertisement contributes to the sales of a product. Causality, with its focus on cause and effect relationships, plays a crucial role in such analyses. It helps to distinguish between the different influences and how they interact with each other. Understanding causality is crucial for effective resource allocation, strategic planning, and evaluating interventions. Causal Inference: The Pathway to Understanding Causality To tackle these problems and answer these questions, reliance is often on causal inference, a set of statistical methods used to estimate causal relationships from data. It uses techniques such as randomised control trials, instrumental variables, and difference-in-differences, among others. Causal inference enables separation of correlation from causation and to investigate the impact of specific factors while controlling for others. It’s a crucial tool in economics, epidemiology, social sciences, and increasingly in machine learning, improving understanding of the inner workings of complex models and making them more interpretable and reliable. In conclusion, the science of causality provides a rigorous framework for understanding the world, allowing us to address ‘what if’ questions and solve attribution problems. It allows us to understand the complex network of cause-effect relationships that determine our reality, providing invaluable insights for decision-making and policy planning. Whether we’re trying to discern the impact of a marketing campaign on product sales, understand the potential outcomes of a policy, or even predict the effects of climate change, causality provides the tools and concepts necessary to parse through these complex problems. While still a challenging field rife with theoretical and practical complexities, the journey into the realm of causality is a necessary one for those wishing to unveil the mechanics of our interconnected world. From counterfactual conundrums to attribution puzzles, causality shines a light on the pathways that help us answer the most important question\u200a—\u200a“Why?” Enhancing AI through the Lens of Causality AI, specifically in the realm of machine learning, has made substantial strides in predictive accuracy. However, the so-called “black box” problem often shrouds the internal workings of these models, making it difficult to understand why a model made a particular prediction. Here, the science of causality can offer a profound contribution. Consider a machine learning model tasked with predicting patient outcomes based on their symptoms. A traditional model may identify correlations, such as patients with symptom X often experience outcome Y. But without understanding causality, we’re left wondering: does symptom X cause outcome Y, or is there an unseen factor Z that causes both? Moreover, how would the outcome change if symptom X were treated? Causal Insights for AI: A Framework Weaving causal science into AI can help alleviate such concerns, offering a more reliable, understandable, and ethically aware AI system. Here is a proposed framework: Data Collection and Pre-processing : This first step remains similar to traditional machine learning. We gather and clean the data, ensuring its quality and relevance. However, causal AI requires careful consideration of the variables included, aiming to capture not just correlates but potential causal factors. Building Causal Models : Instead of jumping directly into pattern recognition, we construct causal models, often represented as directed acyclic graphs (DAGs). These models capture our assumptions about how the world works, indicating the cause-effect relationships among variables. Model Training and Refinement : Train the AI model on the collected data, but guided by the causal framework. The causal model can be used to define the architecture of the AI system, influencing the structure of neural networks or the construction of decision trees. Regular machine learning techniques can then be applied to refine the model parameters. Causal Testing and Validation : Test the AI system, looking not just at prediction accuracy but also causal validity. Does the system’s understanding of cause-effect match empirical evidence? If not, the model needs refining, either in its structure or parameters. Interpretation and Decision Making : With a causal AI system, predictions come with an understanding of why they were made. This allows us to interpret results in light of cause-effect relationships, making AI systems more transparent and trustable. Reaping the Benefits: With this framework, we can enhance AI development in numerous ways: 1. Understanding ‘Why’ : In traditional AI, a system might accurately predict that a patient with a certain genetic marker is likely to develop a disease. With causal AI, we can go a step further and understand why that genetic marker leads to the disease, thereby providing a path to intervention. 2. Improved Generalisation : Causal models are more robust to changes in the environment\u200a—\u200aa crucial advantage in real-world applications. For example, a machine learning model trained on past data may fail if the market dynamics change, while a causal model, understanding the underlying cause-effect relationships, can adapt to these changes. 3. Counterfactual Reasoning : A causal AI can handle counterfactual queries like, “What would have happened had we not launched that marketing campaign?” This ability can offer invaluable insights for decision-making processes. 4. Ethical Considerations : By understanding the causal factors behind predictions, we can ensure that AI systems don’t inadvertently discriminate based on sensitive variables like race or gender. In conclusion, while AI has made significant strides in recent years, the inclusion of causal sciences offers an exciting new frontier for AI development. By enabling AI to understand and reason about the world in terms of cause and effect, we can make these systems more reliable, understandable, and ethically responsible. Tooling that can assist with the Causal Insights for AI framework could be as follows: Data Collection and Pre-processing Pandas : A Python library that provides extensive means for data analysis. It allows for reading and writing data between in-memory data structures and various file formats. PyTorch : More than just a deep learning library, PyTorch also offers a wide array of tools for data loading and preprocessing. TensorFlow Datasets : A collection of datasets ready to use with TensorFlow, along with the tools for loading and preprocessing them. Apache Kafka : A distributed streaming platform capable of handling trillions of events a day. It provides real-time data feeds and a robust platform for stream processing. 2. Building Causal Models pgmpy : A Python library for working with Probabilistic Graphical Models. It allows you to create various types of Bayesian and Markov models and perform exact or approximate inference on them. Tetrad : A tool developed by the University of Pittsburgh for creating, simulating, and analysing causal models (particularly DAGs). CausalNex : A Python library that allows data scientists to incorporate their domain knowledge in probabilistic graphical models, learning structures from data and making causal inferences. DoWhy : A Python library that makes causal inference easy. It provides a unified interface for causal inference methods and automatically tests many assumptions, speeding up robustness checks. 3. Model Training and Refinement Pyro : A universal probabilistic programming language (PPL) written in Python and supported by PyTorch. Pyro enables flexible and expressive deep probabilistic modelling, unifying the best of modern deep learning and Bayesian modelling. NeuroCAUSAL : This tool helps uncover the brain’s causal connectivity patterns from functional magnetic resonance imaging (fMRI) data. CausalNets: A Python library that combines causal reasoning and deep learning for interpretable predictive modelling. TensorFlow and PyTorch : Popular frameworks for deep learning that offer comprehensive libraries for Machine Learning and other complex computations. Keras : A high-level neural networks API capable of running on top of TensorFlow. It allows for easy and fast prototyping and supports both convolutional networks and recurrent networks. Scikit-learn : A Python library that provides simple and efficient tools for predictive data analysis. It features various algorithms for classification, regression, clustering, and dimensionality reduction. 4. Causal Testing and Validation EconML : A Python package developed by Microsoft that uses machine learning to build heterogeneous treatment effect models. EconML provides a suite of methods to estimate the effect of a treatment, policy, or intervention. DoWhy: Already mentioned above, it is also useful for testing the causal assumptions and validating the causal effect. CausalML : A Python package that provides a suite of uplift modelling and causal inference methods. The package leverages machine learning algorithms to estimate the Conditional Average Treatment Effect (CATE). CausalImpact : An R package developed by Google. It provides causal inference using Bayesian structural time-series models. The package implements an approach to estimate the causal effect of a designed intervention on a time series. 5. Interpretation and Decision Making Captum : A model interpretability library for PyTorch. It provides a unified and easy-to-use API for feature attribution, and it supports a wide range of state-of-the-art interpretability algorithms. Lucid : A flexible and easy-to-use library for visualising neural networks, allowing us to better understand, interpret, and refine our models. Alibi Explain : This is an open-source Python library aimed at machine learning model inspection and interpretation. The focus of the library is to provide high-quality implementations of black-box, white-box, local, and global explanation methods. SHAP (SHapley Additive exPlanations) : A unified measure of feature importance that allocates the contribution of each feature to the prediction for each sample. LIME (Local Interpretable Model-agnostic Explanations) : An approach to explain predictions of any machine learning classifier or regressor in a faithful way, by approximating it locally with an interpretable model. What-if Tool : Developed by Google, it provides an easy-to-use interface for understanding a trained machine learning model. It allows you to visualise model results and perform counterfactual analysis. The suggested libraries offer capabilities for each step of the process, enabling the development of transparent, ethical AI systems powered by causal insights. This visual framework provides an actionable guide for implementing causal AI in practice. There also exist proprietary platforms that have integrated some of the tooling to provide a joined up experience for exploring causality and developing explainable AI solutions. One of those platform providers is Geminos.AI. Another such platform is causaLens. Geminos.AI’s Causeway platform uses causal models to remove complexity and increase explainability by providing a common language between data scientists and the business underpinned by the science of causality. To learn more about this topic and explore relevance to use cases relevant to you, do feel free to reach me at mondweep@bridgeconnect.biz. At CBS Consulting, we act as leaders in delivering sustainable change programmes. Disclosure: Although this article has leveraged Generative AI capabilities from Claude 2 and ChatGPT4.0, I have validated the content based on my study and research of the topic. Citation has been included below. To delve into the details, here are some useful references: A Crash Course in Causality: Inferring Causal Effects from Observational Data, University of Pennysylvania (Coursera link, https://www.coursera.org/learn/crash-course-in-causality ) Causal AI, book by Robert Osazuwa Ness https://www.manning.com/books/causal-ai ) By Mondweep Chakravorty on July 21, 2023 . Canonical link Exported from Medium on January 13, 2025.', 'date': 'July 21, 2023'}, {'title': 'Adṛśya\u200a—\u200aWhat if you could simplify Data Sharing Challenges?', 'content': "Adṛśya\u200a—\u200aWhat if you could simplify Data Sharing Challenges? Adṛśya\u200a—\u200aWhat if you could simplify Data Sharing Challenges? If you are a technology innovation company, you would relate to challenges getting your hands on the right data necessary to fuel the… Adṛśya\u200a—\u200aWhat if you could simplify Data Sharing Challenges? If you are a technology innovation company, you would relate to challenges getting your hands on the right data necessary to fuel the innovation drive. If you are a major institution who wants to promote innovation, you can relate to the various regulatory restrictions (for the right reasons) in place to prevent sensitive data from being compromised. Even if data sharing agreements are signed, it still isn’t always possible to maintain full control, which ensures data is often masked or redacted prior to sharing with a third party. Example of redacted\xa0data Example of masked\xa0data As you would expect, masking data limits the degree of product refinements\u200a—\u200athis is particularly true where a key product output may be analytics, which is increasingly becoming relevant in a world of Cognitive AI applications. There have been significant research across the world to optimise the relationship between data integrity (ie retaining the property of the underlying data) and data privacy. Right from our high school mathematics lessons, we have learned that the key properties of a data set are mean, median and mode. Statistical principles allow us to proxy or represent qualitative data like categorical variables (eg. gender, direction, years of certain events etc) too\u200a—\u200afor example, by use of ‘dummy variables’. If we have a representative sampl e, we can utilise the data properties to build analytical toolkit with the right level of predictability. When sharing dataset, algorithms could replace sensitive data with the underlying properties (eg averages/ means) describing the entire data set. So, why isn’t it really used more widely in practice? A face is exposed…… To answer this, let me tell you the story of AOL from 2006. “For the greater good”, AOL ‘ anonymised ’ and released 20 million search queries from 650k users covering 3 months of activities. Researchers used the data set and using triangulation techniques covering a series of search enquiries - ‘ numb fingers, 60 landscapers in Lilburn, Ga and dogs that urinates on everything ’\u200a—\u200aof a specific user (#4417749) exposed a face\u200a—\u200athat of Thelma Arnold, a sixty-two-year-old widow! Read more about the story here . Over the following 13 years, and the proliferation of our data over social media; it has arguably become much easier for data from different places to be triangulated and individual privacy compromised. Hence, it is justified that there is increasing concerns and restrictions on data sharing. I now take the opportunity to introduce Adṛśya , a product innovation that simplifies the data sharing challenge\u200a—\u200aallowing to organisations to build controls over shared data sets to protect and preserve privacy concerns but still allowing meaningful, relevant insights to be drawn from data analytics. Before I describe how Adṛśya achieves this and the use cases it fits into, I would like to expand on three key data concepts that protect or compromise individual privacy. Personal identifier —\u200aAttributes that uniquely identifies an individual: eg someone’s name, phone number Quasi-identifier —\u200aAttributes which if combined with other publicly available datasets can uniquely identify an individual\u200a—\u200aeg address, race Sensitive attribute —\u200aResults, conditions related to an individual\u200a—\u200aeg specific disease, salary, search queries, online ratings If someone is able to triangulate publicly available data to gain access to such information, they can exploit (“inference based attacks”) without necessarily having the individual's consent to use that information. In the example on the left, quasi- identifiers age, sex and postcode from a patient’s database were correlated with public datasest to identify that Bob suffers from Bronchitis! This would allow for example a drug company to advertise respiratory treatment directly to Bob -without his realising how they managed to get to him!! Adṛśya has been developed to prevent shared data from being subjected to such attacks. Adṛśya utilises its proprietary ‘overlapping bucket’ algorithm to allows organisations protect the personal, quasi-identifiers and sensitive attributes of data prior to sharing the data set with a third party. Adṛśya’s ‘overlapping buckets’ algorithm minimise the ‘distance’ between the quasi-identifiers of a data set; whilst at the same time ensuring that the possibility of identifying sensitive attributes are managed within strict probabilistic limits. Adṛśya also provides a selection of other industry algorithms for organisations to use to suit their specific anonymisation needs. Some of these algorithms include k-anonymisation, l-diversity, t-closeness, beta-likeness. Tests have demonstrated that Adṛśya’s proprietary algorithm performs better in reducing the loss of data utility from anonymised data sets. Key features\xa0Adṛśya: Anonymise the same data set for different confidentiality levels\u200a—\u200aeg Internal Departments, Suppliers, Customers, Regulators Allow meaningful analytics for intended purposes only\u200a—\u200awithin the desired security clearance levels and expected collaboration levels Balance data utilisation and data privacy concerns\u200a—\u200aprevent misuse of sensitive attributes, in particular, less frequent ones Below I include a few test results of how Adṛśya’s proprietary ‘overlapping bucket’ algorithm (referred to as ‘ Incognito ’ in the results below) performed/ scaled against other standard algorithms. The comparison covers time taken by various algorithms to anonymise large data sets, how the anonymisation process scaled, and what the resultant data utility loss looked like. Algorithm Legend Time taken by various algorithms to anonymise large data\xa0sets Scalability\u200a—\u200ausing a 5GB data\xa0set Loss of data utility against different anonymisation algorithms The Adṛśya team has found t-closeness to be the poorest in preserving data utility. This is because the t-closeness algorithm prevents semantically similar sensitive attributes of a data set from being bucketed/ group together. In the process, it limits close quasi-identifiers from being grouped together too. Quasi-identifiers are the ones that analytics algorithms rely on the most to generate insights. Adṛśya’s proprietary ‘overlapping bucket’ Incognito algorithm performs much better as can be seen by its closeness to analytics results when using original data… To know more about Adṛśya or if you have requirements to consider data set anonymisation, please feel free to contact me at mondweep@bridgeconnect.biz Thank you for your attention! Feel free to leave a comment or ask if any questions. By Mondweep Chakravorty on March 19, 2019 . Canonical link Exported from Medium on January 13, 2025.", 'date': 'March 19, 2019'}, {'title': 'Smart Energy Transition and Edge Computing', 'content': 'Smart Energy Transition and Edge Computing Smart Energy Transition and Edge Computing Digital technology, pervasively, is getting embedded in every place: every thing, every person, every walk of life is being fundamentally… Smart Energy Transition and Edge Computing Digital technology, pervasively, is getting embedded in every place: every thing, every person, every walk of life is being fundamentally shaped by digital technology\u200a—\u200ait is happening in our homes, our work, our places of entertainment. It’s amazing to think of a world as a computer. I think that’s the right metaphor for us as we go forward.\u200a—\u200aSatya Nadella, CEO of Microsoft The internet is no longer a web that we connect to. Instead, it’s a computerized, networked, and interconnected world that we live in. This is the future, and what we’re calling the Internet of Things.\u200a—\u200aBruce Schneier, security technologist, and author With the world undergoing an energy transition driven by the perils of climate change, this paper discusses how developments in edge computing through the internet of things could drive innovations to create a circular economy . As per a survey of worldwide telecommunications professionals, the Utilities and Industrial IoT show great market potential. In addition, over 70 percent of respondents worldwide agree that the use of smart technologies in the home makes life easier. Smart devices are a type of IoT enabler for edge computing to facilitate the energy transition. Some of the applications and benefits of IoT in the energy sector include: Demand response for residential, commercial &industry users reducing demand at peak time; which itself reduces the grid congestion and electricity bills and the need for investment in grid backup capacity Smart building for centralised and remote control of applications and devices improving readiness to join a smart grid or virtual power plan and improving the integration of distributed generation and storage systems Battery energy management leading to data analytics for activating battery at the most suitable time reducing the cost of overall energy use Various applications of IoT in the Energy Utilities industry Retail electricity customers appear to be developing an overall positive attitude towards smart devices, as evidenced by this relatively recent survey of Finnish energy consumers. The rest of this paper presents an architectural pattern of how smart/ home IoT devices could leverage technological developments in real-time event streaming and enablers such as 5G and FTTP to construct an accurate picture of energy utilisation at homes. Energy companies can then apply the insight for both supply side as well as demand-side management. For example, drive efficiencies in energy trading, and consumption. For illustration purposes, the discussion below borrows illustration from Google’s IoT Core product, which is a fully managed service that allows a user to easily and securely connect, manage, and ingest data from millions of globally dispersed devices. Energyworx for example has used the IoT Core platform to build an energy data management platform that enables behavioral science and lets businesses aggregate data in several profiles with valuable context-enriched data points.GCP’s Reference Architecture for IoT Core Fig 1. Google IoT Reference Architecture Cloud IoT Edge\u200b is a set of software packages and services that turns Linux based devices into full-featured IoT edge devices capable of running and applying ML models at the data source. Devices can be anything for many different use cases\u200a—\u200afrom patient-centric medical devices to robotic arms, oil rigs, wind turbines, and the like\u200a—\u200aany device for which it makes sense to apply machine learning in real-time, onboard the device. At the hardware level, Cloud IoT Edge devices can include one or more Edge TPUs (tensor processing unit\u200b). An Edge TPU is an extremely small hardware accelerator ASIC(application-specific integrated circuit)developed by Google for the TensorFlow \u200bframework. Open-sourced by Google, the TensorFlow API and framework are used for machine learning applications and neural networks. (Cloud ML Engine\u200a—\u200a\u200b the serverless, a fully managed core component of the Google Cloud Platform that is the centerpiece of Google’s machine learning capability for GCP\u200a—\u200acan be thought of as hosted TensorFlow.) Several Edge TPU chips can fit within the circumference of a penny, so even the smallest devices can be equipped with machine learning capabilities. Read more about it here . Non Invasive load monitoring (NILM) techniques are traditionally used to disaggregate energy consumption within a property using transient and steady-state device signatures (Fig 2). Fig2. Transient and Steady-State noise signatures used in NILM techniques for load disaggregation However, it isn’t as effective where the devices have complex states like washing machines, dishwashers. One reason for that is that those devices change states more frequently than the usage data (for example half-hourly) captured by the (smart) meters. For example, refer to the illustration below. Fig 3a. Device signatures Fig 3b. Half hourly smart meter data profile (illustration only) Here we discuss how smart IoT capabilities could complement the process by enabling more accurate device state capture in near real-time. In this reference architecture, devices would publish their telemetry state information (eg what state of the cycle a dishwasher or a washing machine is) in real-time to a topic (Fig. 3). Fig 3. A simple example of IoT data flow between edge devices and a\xa0server Fig 4. Device configuration and Telemetry Data analytics techniques could then Join the device state information with industry-wide NILM databases of the device signatures and device fingerprints to more accurately infer its usage energy usage data. Google Cloud IoT Core enables end devices to be configured centrally through a device manager across the internet. Superimpose the disaggregated energy information to smart meter energy data to generate insights into energy usage habits of households and industries Energy companies can recommend energy-saving tips and promote incentives to influence energy consumption habits. Monitoring energy consumption and other parameters are estimated to allow for a 4 0% reduction in energy consumption . A few themes to innovate on top of an IoT enabled energy data management system Household device usage rule engine : Households could set rules defining when specific devices could be used and for how long. They could remotely configure usage patterns of light bulbs, fans, televisions, etc for example when they are away from the property Device Hub : A digital control panel for household and commercial users easy access to their device configuration\u200a—\u200afor example usage statistics, the ability to set themes (such as lighting, etc)by rooms My Offers : Presenting customers Offers (eg cashback, etc) if certain target usage patters are met (eg not using washing machines and dishwashers during peak demand period) Thank you for your time and if you would like to add to the discussion, please feel free to leave your comments or connect with me over LinkedIn . Other References: Building a smart home cloud service: https://medium.com/google-developers/building-a-smart-home-cloud-service-with-google-1ee436ac5a03 (PDF) Intelligent Edge Computing for IoT-Based Energy Management in Smart Cities In recent years, green energy management systems (smart grid, smart buildings, and so on) have received huge research… www.researchgate.net 2.https://www.researchgate.net/publication/338684011_Internet_of_Things_IoT_and_the_Energy_Sector/link/5e244ecc299bf1e1fabdce64/download 3. https://www.youtube.com/watch?v=X_T8KDQLe2Q By Mondweep Chakravorty on December 18, 2020 . Canonical link Exported from Medium on January 13, 2025.', 'date': 'December 18, 2020'}, {'title': 'Blockchain basics\u200a—\u200aElliptic Curve Cryptography', 'content': 'Blockchain basics\u200a—\u200aElliptic Curve Cryptography Blockchain basics\u200a—\u200aElliptic Curve Cryptography In The Matrix, The Keymaker controls access to key information. Which includes access to the door leading to The Architect. In the video… Blockchain basics\u200a—\u200aElliptic Curve Cryptography In The Matrix, The Keymaker controls access to key information. Which includes access to the door leading to The Architect. In the video clip above, The Keymaker tries to hand over a key integral for Neo’s path as The One to Niobi. And you might be wondering what does this have to Elliptic Curve Cryptography (ECC)?! Well, let’s start with the basics of public key cryptography\u200a—\u200awhich means that it is easy to secure information by keys that are based on mathematical functions that have a special property: it is easy to calculate them, but hard to find out how they were calculated. For example, the number 8,018,009 is the product of two prime numbers. It is trivial to generate it if the two prime numbers were known. However, unless one of the numbers (2003)is known, it is more difficult to derive the second (4003). Such functions are called trapdoor functions because they are difficult to reverse without prior knowledge of one piece of secret information. RSA ( Rivest–Shamir–Adleman ) is a public-key cryptosystem that is widely used for secure data transmission. However, in the energy intensive world of blockchain technology, it is not secure enough\u200a—\u200aRSA is still a trapdoor function and can be compromised without a sufficiently large key chain. Blockchain technology instead uses a more advanced category of mathematical functions based on arithmetic operations on an elliptic curve for which no known trapdoors are currently there. breaking a 228-bit RSA key requires less energy to than it takes to boil a teaspoon of water. Comparatively, breaking a 228-bit elliptic curve key requires enough energy to boil all the water on earth. A comparative view of RSA and ECC is nicely presented here and to visualise how much harder it is to break these encryptions, check out at your leisure the concept of Global Security from Lenstra here —\u200anote it is a slightly technical read. In the section below, we will go a bit more into the details of how private and public key pairs are generated using ECC; and how hash functions are used with the public key as input to support a number of use cases. The base relationship between a private key (k) and the public key (K) is: K =\xa0k*G k is any integer of 32 bytes (ie 256 bits). This can be generated for example by flipping a coin 256 times and recording the outcome (1 for Head and 0 for Tail). This serves as the private key and should be kept a secret by who generates it. G is a point on the Elliptic Curve [y²= (x³+7) over a (finite) field of prime order p, where p = 2²⁵⁶- 2³²- 2⁹-2⁷-2⁶-2⁴-1] The resulting curve is visually as a collection of points (shown in the image)\u200a—\u200ait isn’t a continuous curve as the values of x and y to satisfy this equation can only be a series of discrete numbers only. The equation is solved by perfoming modulus division\u200a—\u200athe equation above can be re written as y² mod p = (x³ + 7) mod p. The operator for this in python is % Elliptic Curve\xa0Points One property of elliptic curves is that two points on the curve are added, the result is also another point in the elliptic curve. This means that if an integer k (acting as the private key) is multiplied with a discrete point G on the curve, the result is equal to G+G+…(k times) and it is a point again on the curve. The result K is called the public key and can be shared with others to access relevant encrypted information. Another property of the elliptic curves is that whereas K = k*G is valid; the reverse operation is not possible\u200a—\u200aie k\xa0!=K/G. So whereas the same point G is used in all programme implementations (eg OpenSSL , libsecp256k1 ) and combined with a random number k to generate a user’s public key K; it is not possible for anyone to perform a division of the published public key K with the (known and common value) of G (the generator point) to derive someone’s private key. This is the most important feature which enables the use of Elliptic Curve arithmetic for public key cryoptography. In Ethereum, public keys are used to generate a number of related features. For example addresses that would seem like IBAN bank accounts ( ICAP ) and digital signatures. To understand how this happens, it is necessary to understand a feature called hashing . A hash function is loosely speaking a map\u200a—\u200ait maps an input data of arbitrary size to an output data of fixed size. The hashing function demonstrates a few key properties: Determinism -A given input message always produces the same output Verifiability\u200a—\u200aComputing the hash of a message is efficient Non correlation\u200a—\u200aA small change in the input message (even the least significant bit) produces a very different hash output; so it cannot be related to original input Irreversibility\u200a—\u200aComputing a message from its hash is not feasible\u200a—\u200aie brute force search is not possible Collision protection\u200a—\u200aIt should not be feasible for two input messages to result in the same hash output Due to these properties, hashing allows a variety of use cases such as data fingerprinting, message integrity, authentication, unique identifiers etc. In Ethereum the keccak-256 algorithm is used to achieve hash outputs. Now back to The KeyMaker from The Matrix. In the Ethereum world, The KeyMaker would be the wizard of keccak-256 and perform the magic and generate the various addresses\u200a—\u200aexternally owned accounts (ie those that hold Ether) and smart contracts (those that contain business functionality) published on the blockchain. So, were a virtual world be create in the MetaVerse and humans’ consciously transferred over to there; The KeyMaker would be crucial for Neo to extract someone else from there. Or was it why the NSA is alleged to have placed a backdoor in the Dual_EC_DRBG ramdon number generator when the National Institute of Science and Technology attempted to standardise Keccak as part of the Federal Information Processing Standard (FIPS) 202 in 2015?! Read more about it here … I hope this article has been useful to you\u200a—\u200aplease feel free to leave comments and also what you would like to know more. I will continue with further posts with The Matrix as the inspiration for learning more about blockchain. By Mondweep Chakravorty on February 14, 2022 . Canonical link Exported from Medium on January 13, 2025.', 'date': 'February 14, 2022'}, {'title': 'Driving Agility: Maximising Value', 'content': "Driving Agility: Maximising Value Driving Agility: Maximising Value I was recently asked how I would maximise value generation in a multi-team digital development environment. When responding to the… Driving Agility: Maximising Value I was recently asked how I would maximise value generation in a multi-team digital development environment. When responding to the question, I drew upon my professional training and experiences to draw up a sketch of the context and meaning of (a) what is ‘value’ (b) what some of the key principles and enablers of value creation or value loss are; so that (c) those responsible for value generation remain aware of and can measure the outcomes and benefits. My intention is to articulate the same here and seek your opinions and draw upon your experiences of how you may relate to it. Before we go into the specifics, I would like to start by stating that the principles that I attest to are: responsibility, empowerment, transparency, and accountability. For with responsibility comes accountability, but for responsibility to be credible requires (feelings of) empowerment\u200a—\u200awithout which the stage isn’t really set up for success. Responsibility, as well articulated in the accompanying short video, promotes empathy, and a drive to appreciate the bigger picture so that one feels and is empowered to make effective and timely decisions to do a brilliant job than seeing it as a series of tasks. “The price of greatness is responsibility.” ~ Winston Churchill. Next, let us consider for a moment what ‘Value’ is and some example of value creation and value destruction. ‘Value’ can be subjective\u200a—\u200aas Marty Roberts sang “ One Man’s Trash Is Another Man’s Treasure ”. For ‘Value’ to be objective therefore, the context is important. Let’s consider an enterprise that sees a very attractive opportunity with a government regulator dangling the carrot to upgrade the national infrastructure and technology to improve inclusivity, accessibility, and choice for the population. If so, how does the firm align its corporate strategy (the why, the what and the how) to march ahead in support of the business case, stay ahead of the competition, design factors to differentiate its product proposition to appeal to the most appropriate market segments. All of these thoughts would crystalise into a chain of activities to generate a set of coherent capabilities that could be leveraged to drive suitable outcomes in support of the business case. In management parlance, the chain of sustainable activities is variously called Value Chain or Value Streams and quite simply are a series of logical steps to achieve an objective. In the example, we are discussing the value chains for the relevant journeys of acquiring new customers and upselling to existing customers could look as shown below. a. Acquire New Customers: Advertise Products > Generate Leads > Engage With Prospects > Acquire New Customers > Positive Early Onboarding UX (setup, interaction)> Provide Ongoing Simple UX (eg accessibility, billing) > Provide Upgrade / Downgrade Options > Drive Loyalty b. Leverage Existing Customer: Generate Awareness > Provide Upsell Options (eg Promotions) > Provide Improved UX (e.g. accessibility, home move, price transparency) > Drive Ongoing Loyalty Our objective should be to align our ducks to drive responsibility and empowerment so that while working within the constraints of budget, and schedule; the essential capabilities could be scoped in and to generate the desired outcomes and benefits. In more traditional ‘ways of working’, this is done through a series of projects whose focus is to generate a series of outputs (eg a new channel or an improved billing system) in the expectation that they would be coherent. Unfortunately, the ‘project specific’ thinking results in a temporary organisation for a specific set of changes\u200a—\u200ainadvertently, leading to siloed and disconnected ways of working across an enterprise. Significant management effort is necessary to broker relationships between teams resulting in inefficiencies and creating distance between the business objectives and those who are meant to be responsible for enabling the business. Given that projects are essentially temporary, there aren’t always opportunities to build long term working relationships amongst team members and the behaviour, in general, is reactive rather than participatory and proactive. In contrast, a ‘product based’ thinking/ approach funds a set of teams on an ongoing basis to incrementally build and improve capabilities to support the business objectives\u200a—\u200aencouraging direct, improved collaboration and continuity between business and technology teams as well as joint ownership, responsibility and empowerment to determine and negotiate sensible changes to support the value streams. In our fictitious example, assume that the customer preferences are unlikely to be as yet well understood for the new proposition, and so rapid feedback is necessary for success. With possibilities of substitutes becoming viable in the medium term, creativity is necessary to generate effective value propositions. In the context of the developing circumstances, it is, therefore, more valuable to ‘fail faster’ and recover from it. Under such a circumstance, a ‘product based’ approach is more valuable than a mere ‘project-based’ approach. Do refer to this well-written article in martinfowler.com too on the key differences between project and product based approaches. Referring back to the value streams for our sample example attempting to leverage the industry drivers, an effective approach would be to organise itself vertically into a set of relevant “tribes” or “journeys” aligned to the value generating streams\u200a—\u200aassume two, one with the mandate to acquire new customers and another to upsell to the existing customers\u200a—\u200aas the firm builds and rolls out its new offerings. The tribes could organise themselves as squads reflecting the agreed value streams within the journey definition and work closely with empowered product owners from the business to generate products that can dovetail with capabilities; either existing or new, to allow benefits to be generated and measured in much shorter cycles. As the squads are aligned to the steps in the value streams and are cross-functional, with deep domain and technical expertise, they are able to adapt to changes in relevant drivers (for example market or regulatory) to correct course and continue generating the most optimal outputs. The “tribes” are masters of their own destiny and demonstrate tribal behaviours — controlling their own budget and self-organising themselves instead of being dependent on external factors like their share of the sum of “estimated” budget amounts of a number of projects, that by their own nature are temporary and may lose strategic relevance for the organisation*. If an organisation still likes to drive change through projects, the projects can pitch the changes to the tribe or journey leads; with the decisions being the outcome of a negotiation between the portfolio/ programme function and the tribe journey leads. As final thoughts, I would also like to share the following insights from Mckinsey & Company about essential changes to operating models to achieve true agility to drive value maximisation. Use case reference : ING’s Agile Transformation ING's agile transformation Established businesses around the world and across a range of sectors are striving to emulate the speed, dynamism, and… www.mckinsey.com By Mondweep Chakravorty on July 3, 2019 . Canonical link Exported from Medium on January 13, 2025.", 'date': 'July 3, 2019'}, {'title': 'Blockchain basics\u200a—\u200aNFTs (Non Functional Tokens)', 'content': 'Blockchain basics\u200a—\u200aNFTs (Non Functional Tokens) Blockchain basics\u200a—\u200aNFTs (Non Functional Tokens) In the Matrix movie, Morpheus offers Neo the blue or the red pill. Neo chooses the red pill to know the truth which eventually leads him… Blockchain basics\u200a—\u200aNFTs (Non Functional Tokens) In the Matrix movie, Morpheus offers Neo the blue or the red pill. Neo chooses the red pill to know the truth which eventually leads him to wake up in the real world. Now, you might wonder, what does this have to do with Blockchain and NFTs? Well, I would say the act of accepting the red pill was transfer of ownership of Neo’s consciousness to Morpheus, allowing Morpheus to trace Neo’s consciousness from the metaverse of the Matrix to locate his body and consciousness in the real world! NFTs or Non Fungible Tokens are as the name says “non fungible”. They represent a unique tangible or intangible items and are therefore not interchangeable. For example, a token that represents ownership of a specific Mozart composition is not equivalent to another token that represents ownership of a Beethovan composition. They may be issued by the same “music composition ownership token” system; but each token is unique (with its own serial number) and not comparable or transferable with one another. Now, in the movie, Neo is offered a choice\u200a—\u200aif he takes the red pill, he consents to seeing the real world; if he instead chose the blue pill, he would have no memory of the encounter with the hackers (Morpheus, Trinity and crew) and wake up happy in his bed. This probably captures the Counterparty Risk element in a token economy. Whereas in theory, any asset in the real world can be represented by a digital token and ownership transferred through the token; there is an additional risk that the custodian of the actual asset may not recognise (or allow) transfer of ownership based on the transfer of a token (such as a certificate, deed, title or digital token). Whereas Counterparty Risk would not exist for assets intrinsic to the metaverse (like a digital plot of land in the that HSBC recently bought in TheSandbox metaverse), it does exist for assets in the real world (like a painting or a car). Token issuers and owners may still depend on real-world, non-smart contracts (outside the blockchain environment) for the extrinsic assets. Incorporating the custodian into a blockchain (intrinsic)organisation or DAO (Decentralised autonomous organisation) does remove counterparty risk though for extrinsic assets too. There has been a proliferation of NFTs in recent years. I have shared a few references in the Appendix section for those curious to read about some of those. The judge is still out though whether NFTs are a fad or add value. NFT mining like any other crypto transactions are energy hungry. The Ethereum platform where a number of NFTs are issued and transacted is estimated to require all of the amount of energy that the whole of Zimbabwe to operate. The assets (like paintings etc) that NFTs link ownership to are visible online; the NFT is currently just receipting ownership to those assets. I would argue though that the real value of NFTs would be realised as the metaverse takes shape and is more widely adopted. Then the owner of the NFT for example of the Mona Lisa painting or the Kohinoor Diamond may earn royalty for allowing the digital representation and experience in the metaverse! If you enjoyed reading this piece and it helped you learn something more about NFTs, do drop a like, clap or a comment. Now, back to reflect on Neo and the blue pill… In the Matrix , Thomas Anderson works as a computer programmer while maintaining a double life as a hacker, under the alias “Neo”. He is restless and driven to learn the meaning of cryptic references to “the Matrix” appearing on his computer. Infamous hacker Trinity contacts Neo and informs him that a man named Morpheus can tell him what the Matrix is; however, the three Agents arrest Neo to prevent him from collaborating with Morpheus. Undeterred, Neo meets with Morpheus and confirms that he wants to learn more about the Matrix by choosing an offered red pill. After swallowing the pill, Neo abruptly awakens in a liquid-filled vessel, connected along with millions of other people to an elaborate electrical structure. He is rescued by Morpheus and brought aboard a levitating ship, the Nebuchadnezzar. Appendix: Some useful references on NFTs: 50+ Incredible NFT Statistics (2022) by Josh Howarth - March 16, 2022 This is a list of interesting 50+ non-fungible token (NFT) stats worth knowing. 2021… explodingtopics.com NFTs in 2021 by the Numbers For those who have yet to take the plunge into NFTs, the information being disseminated throughout the newly robust… nftnow.com Use of NFTs to support scientific research: https://www.nature.com/articles/d41586-021-01642-3 By Mondweep Chakravorty on March 16, 2022 . Canonical link Exported from Medium on January 13, 2025.', 'date': 'March 16, 2022'}, {'title': 'Tracking Electricity Consumption data with a ChatGPT AI assistant', 'content': "Tracking Electricity Consumption data with a ChatGPT AI assistant Tracking Electricity Consumption data with a ChatGPT AI assistant Beyond the hype…. Tracking Electricity Consumption data with a ChatGPT AI assistant Beyond the hype…. I have been exploring the use of ChatGPT across topics ranging from understanding legal text, drafting contracts\u200a—\u200aboth traditional and smart ones, and generally refreshing my software coding knowledge. Since yesterday evening, however, I have used it to create something real and meaningful for me. What prompted this was a surprise bill from Octopus Energy\u200a—\u200athe bill said I owned them an abnormal amount of money in one month’s electricity consumption. Of course, they provide an in home display and access to MyAccount; however, they aren’t necessarily designed to alert me if my daily consumption exceeds a certain threshold. So, I asked ChatGPT to help me code an application. Now, some disclaimer\u200a—\u200amy software coding skills have been rusty. I don’t code on a daily basis as my job is more managerial; but I know the fundamentals of the programming languages. So I took a plunge to use ChatGPT to help me. The end result is an app that Tells me about my energy consumption during the day and prompts me whether I have cross my budget threshold during the day (though for reasons shared below, it doesn’t get the data to do so on a real time basis) (figure 1) Tells me about my current (ie last half hourly) energy consumption (figure 2) Creates a plot of all half hourly energy consumption (figure 3) Figure 2 Figure 3 In the process, I managed to learn with a hand on approach not only a bit more about python, javascript, git and Flask; but also about how Octopus’ APIs are written and how to use them. I also learnt that although Octopus’ API allow us to get information about our energy consumption; it is not real time\u200a—\u200athe available information lags by almost 24 hours. So, creating an application to support real time use cases is not immediately available So, you may wonder this is great but why is this experience relevant to those who don’t code but manage software development teams and programmes. The simple reason is that it allows anyone with an idea to translate the idea into a working software. Doing so makes it easier to communicate the potential of the idea with a working software and iteratively refine it by engaging the experts\u200a—\u200abe it UX developers, software engineers, business analysts, or sponsors. I am on the opinion that ChatGPT has quite a lot of relevance for anyone who wants to design a new product. Your thoughts and opinions are welcome. I share below some reference on how I progressed. References: My initial prompt to ChatGPT was Octopus Energy provide my electricity. I have a smart meter. I want to set up a daily budget and track against it. If I near 80% of my daily budget, I would like to get an alert. Please write me a code that allows me to authenticate myself with Octopus Energy, retrieve the smart meter data and achieve my objective above. 2. ChatGPT responded with the following starting response 3. It took some tinkering and troubleshooting to create a webapp out of it. The code that was finally written for the webapp is in the git repository here\u200a—\u200aI have removed my personal details; but otherwise it should work for you. GitHub - mondweep/electricity-alert-public at master You can't perform that action at this time. You signed in with another tab or window. You signed out in another tab or… github.com 4. Octopus Energy API documentation => https://developer.octopus.energy/docs/api/ By Mondweep Chakravorty on May 6, 2023 . Canonical link Exported from Medium on January 13, 2025.", 'date': 'May 6, 2023'}, {'title': 'Banking Payment Systems — Part 1', 'content': 'Banking Payment Systems — Part 1 Banking Payment Systems — Part 1 Banking Payment Systems\u200a—\u200aPart\xa01 Banking Payment Systems - Part 1 4th July 2019: Whilst going through my Articles on LinkedIn, I noticed this one that I had drafted in March 2018 but… www.linkedin.com By Mondweep Chakravorty on July 4, 2019 . Canonical link Exported from Medium on January 13, 2025.', 'date': 'July 4, 2019'}, {'title': 'When I tried with an Europe region, I got an error as shown below.', 'content': 'When I tried with an Europe region, I got an error as shown below. When I tried with an Europe region, I got an error as shown below. gcloud beta iot registries create weather-station-registry — region europe-west1-b — event-pubsub-topic=telemetry-topic When I tried with an Europe region, I got an error as shown below. The error goes away if I use the US region as in your instructions. Would you know why perhaps? gcloud beta iot registries create weather-station-registry\u200a—\u200aregion europe-west1-b\u200a—\u200aevent-pubsub-topic=telemetry-topic ERROR: (gcloud.beta.iot.registries.create) PERMISSION_DENIED: The caller does not have permission Vs gcloud beta iot registries create weather-station-registry\u200a—\u200aregion us-central1\u200a—\u200aevent-pubsub-topic=telemetry-topic Created registry [weather-station-registry]. By Mondweep Chakravorty on January 20, 2018 . Canonical link Exported from Medium on January 13, 2025.', 'date': 'January 20, 2018'}, {'title': 'Hi — thank you for the article.', 'content': 'Hi — thank you for the article. Hi — thank you for the article. Thank you for clarifying. I am working to teach my daughter and introduce her to the world of programming. Hi\u200a—\u200athank you for the article. Can I ask if the following kit would be sufficient for the setup. I could not be sure because the schematic had reference to a DHT sensor / GPIO04/D2 Thank you for clarifying. I am working to teach my daughter and introduce her to the world of programming. By Mondweep Chakravorty on January 20, 2018 . Canonical link Exported from Medium on January 13, 2025.', 'date': 'January 20, 2018'}, {'title': 'I connected the following kit to my macbook and attempted the suggested command.', 'content': 'I connected the following kit to my macbook and attempted the suggested command. I connected the following kit to my macbook and attempted the suggested command. However, it fails with message “Error: exit status 2 I connected the following kit to my macbook and attempted the suggested command. http://osoyoo.com/2017/05/03/nodemcu/ However, it fails with message “Error: exit status 2 Error: build failed” So, I am wondering if there should be any pre checks I should do. When connecting the hardware to my macbook with the USB cord, I do notice that a small LED on the board just flashes once and doesn’t stay on afterwards…. Thank you for your inputs. By Mondweep Chakravorty on January 21, 2018 . Canonical link Exported from Medium on January 13, 2025.', 'date': 'January 21, 2018'}, {'title': 'Cybersecurity Assessments', 'content': 'Cybersecurity Assessments Cybersecurity Assessments There is an ever growing plethora of devices in our lives. Have you ever wondered how they are secured and operate to common standards… Cybersecurity Assessments There is an ever growing plethora of devices in our lives. Have you ever wondered how they are secured and operate to common standards? This brief summarising an excellent study by ENISA (European Union Agency for Cyber Security) should help explain… Mind Map: https://drive.google.com/file/d/1TNMveVt-jUiqnWyiYYEGG_04Ko62aICS/view?usp=drive_link Main Report: https://drive.google.com/file/d/1XUbFCi9K2HkRWM2v1gIFqgn6-BP5swNH/view?usp=drive_link By Mondweep Chakravorty on February 9, 2024 . Canonical link Exported from Medium on January 13, 2025.', 'date': 'February 9, 2024'}, {'title': 'On Customer Experience\u200a—\u200a‘Analog’ to Digital', 'content': 'On Customer Experience\u200a—\u200a‘Analog’ to Digital On Customer Experience\u200a—\u200a‘Analog’ to Digital Lessons from Exploring Phones in a Store On Customer Experience\u200a—\u200a‘Analog’ to\xa0Digital Lessons from Exploring Phones in a Store I want to play with the phone too…… This image of my son jumping up and down because he cannot reach the phones highlights how high street stores may be missing out on a few tricks to improve overall customer engagement. To appreciate what I mean, notice a few things Empty white spaces below the big display screens on the walls The displays being static\u200a—\u200anot promoting any interactions even for adults The furnishings (geometrically shaped display tables) reducing free movement in the limited floor space A visitor has to wait in most cases 10-15 min to be served by the 3-4 customer service agents in the store Technology could and rather be leveraged to improve the experience derived from this retail store investment. Use of interactive displays\u200a—\u200ausing available technology Provide touch-enabled capabilities in the empty white walls to play games, draw images, explore stories and phone technology to keep kids engaged. They would happily while their time away Allow interactions in the existing displays so adults could self serve- explore the phone models, compare features and possibly order without having to wait for a customer agent to become available first 2. Rearrange the phone model display in one central island\u200a—\u200aso those interested can engage with one another too. People are social and that is likely to improve the popularity of and engagement within the store 3. By rearranging the displays from multiple islands to one central island, space is freed up to demonstrate futuristic technology. For example, deploy Augmented Reality (AR)enabled display of a premium model that customers and visitors can interact with and explore using their existing devices. I would expect this to increase the desirability of both the advertised product and the store experience. Here is an interesting video I came across around AR enablement in grocery shopping\u200a—\u200ait provides some perspective of even how rather dull experiences can be enriched using technology => I feel the points suggested above would transform in-store experiences into rich memories which will allow the high street to use technology to differentiate from the online channel experience. If I had to just check phones, and order them I can do so today from a laptop or phone while at home and without waiting 10-15 min waiting for staff in a store to become available and then possibly telling me nothing that I may not already find online. I welcome your thoughts. video credit: goopy.io By Mondweep Chakravorty on September 22, 2019 . Canonical link Exported from Medium on January 13, 2025.', 'date': 'September 22, 2019'}, {'title': 'Making restaurant menus fun using AR', 'content': 'Making restaurant menus fun using AR Making restaurant menus fun using AR Today, I wanted to talk about something I have thought about in the past - how AR technology could one day add value to the experience of… Making restaurant menus fun using\xa0AR Today, I wanted to talk about something I have thought about in the past - how AR technology could one day add value to the experience of having dinner with my children at their favourite space, Wagamama?! Without a restaurant app yet supporting this featute, I am going to simulate the experience using few available apps on Google Play. Inspite of queuing for a while there was a bit of a delay being seated due to a recent policy of not allowing dogs even in the outside seating area and I thought for a moment that I might not be able to blog about the hypothetical dining experience using AR. A pragmatic solution involved agreeing to sit at the edge of the outside seating area and tethering the dog outside of their marked off area! So now I could continue writing about the topic!! The process of ordering involves either considering the items on a printed menu or on the restaurant app. After the initial meet and greet, we ordered our food using the menu on the table. It is useful to point out that at this stage an ability to see the food on the table before ordering may create a real fun experience - especially with children about. My little one ordered ramen noodles without vegetables and the rest of us our usual favourites. The food arrived shortly afterwards. It is at this point that my little one realised that ramen noodles comes with soup. Of course, it is what the dish is about; but he went by the look of the dish in the menu and didn’t necessarily clarify about the contents. If instead he could see his dish before ordering, wouldn’t that be a wonderful possibility?! In the Google Play store, there are a number of apps that are aiming to serve this need. I explored a few of them and this is a feature in\xa0one\xa0of\xa0them. Of course, the content don’t come close to the looks, aroma and flavours of Wagamama; and I think it is an area that family friendly restaurants like Wagamama should test with their diners. Apart from the fun of ordering food, it could prompt interest groups amongst table - say Chicken Udon Lovers and Katsu Curry with Rice Fans! Not to mention an engaging and immersive channel to showcase the ethically sourced food and how it is prepared through a mix of gamification thrown in for the fun of the diners and the waiting staff. By Mondweep Chakravorty on June 27, 2021 . Canonical link Exported from Medium on January 13, 2025.', 'date': 'June 27, 2021'}], 'topics': ['4417749', 'Agile', 'Ai', 'Architecture', 'Artificial Intelligence', 'Cloud', 'Consulting', 'Data Analytics', 'Deep Learning', 'Digital Transformation', 'Gcp', 'Generative Ai', 'Infrastructure', 'Innovation', 'Leadership', 'Machine Learning', 'Manufacturing', 'Ml', 'Neural Networks', 'Retail', 'Security', 'Strategy', 'Technology'], 'expertise': ['Architecture', 'Digital Transformation', 'Innovation', 'Leadership', 'Management', 'Strategy', 'Technology'], 'content_themes': ['Agile', 'Digital Transformation', 'Innovation', 'Leadership'], 'article_count': 18}, 'analysis': {'core_competencies': {'primary_skills': ['Agile Project Management', 'Incident Management, Security Information And Event Management (Siem)', 'Stakeholder Management', 'Solution Architecture', 'P&L Management', 'Mentoring', 'Product Strategy', 'Client Relations', 'Artificial Intelligence (Ai)', 'Delivery'], 'skill_frequency': {'Agile Project Management': 1, 'Incident Management, Security Information And Event Management (Siem)': 1, 'Stakeholder Management': 1, 'Solution Architecture': 1, 'P&L Management': 1, 'Mentoring': 1, 'Product Strategy': 1, 'Client Relations': 1, 'Artificial Intelligence (Ai)': 1, 'Delivery': 1, 'Information Security Management System (Isms)': 1, 'Social Media Marketing': 1, 'Cyber Security Risk': 1, 'Management Consulting': 1, 'Artificial Neural Networks': 1, 'Strategy': 1, 'Project Finance': 1, 'Business Analysis': 1, 'Project Management Office (Pmo)': 1, 'Instrumental Variable': 1, 'Sales Processes': 1, 'Cloud Strategy': 1, 'Senior Stakeholder Management': 1, 'Azure': 1, 'Cloud': 1, 'Technical Project Leadership': 1, 'Requirements Gathering': 1, 'Leadership': 1, 'Business Strategy': 1, 'Contract Negotiation': 1, 'Automotive': 1, 'Digital Transformation': 1, 'Cybersecurity': 1, 'Management': 1, 'Project Management': 1, 'Team Building': 1, 'Business Development': 1, 'Machine Learning': 1, 'Generative Ai': 1, 'Leading Development Teams': 1, 'Change Management': 1, 'Propensity Score Matching': 1, 'Cross-Functional Team Leadership': 1, 'Rfp Design': 1, 'Cybersecurity Tools': 1, 'Supervised Learning': 1, 'Vulnerability Management': 1, 'Problem Solving': 1, 'Ml': 1, 'Vendor Management': 1, 'Client Liaison': 1, 'Cloud Computing': 1, 'Cybersecurity Incident Management': 1, 'Coaching': 1, 'Agile & Waterfall Methodologies': 1, 'Team Management': 1, 'Programme Delivery': 1, 'Amazon Web Services (Aws)': 1, 'Aws': 1, 'Network Architecture': 1, 'Data Analysis': 1, 'Program Delivery Management': 1, 'Fundraising': 1, 'Google Cloud Platform (Gcp)': 1, 'Information Technology Infrastructure': 1, 'Product Development': 1, 'Program Management': 1, 'Causal Inference': 1, 'Planning Budgeting & Forecasting': 1, 'Payments': 1, 'Stakeholder Engagement': 1, 'Ai For Good Project Framework': 1, 'Product Management': 1, 'Exploratory Data Analysis': 1, 'Causality': 1, 'Integration': 1, 'Cybersecurity Auditing': 1, 'Ai': 1, 'Agile': 1, 'Digital Marketing': 1, 'Requirements Analysis': 1, 'Network Security': 1, 'Product Design': 1, 'Program Development': 1, 'Entrepreneurship': 1, 'Google Compute Engine': 1, 'Machine Learning Algorithms': 1, 'Software Project Management': 1, 'Team Leadership': 1, 'It Strategy': 1, 'Project Planning': 1, 'Virtual Machines': 1, 'Software Development': 1, 'Supportive Services': 1, 'Agile Methodologies': 1, 'Scrum': 1, 'Security Strategy': 1, 'Partnerships': 1, 'Technical Support': 1, 'Jupyter Notebooks': 1, 'Program Leadership': 1, 'Project Delivery': 1, 'New Product Validation': 1}, 'source_distribution': {'cv': 7, 'technical_skills': 61, 'soft_skills': 36, 'linkedin_topics': 0, 'medium_topics': 23}}, 'experience_level': {'leadership_roles': ['Director', 'Head', 'Lead', 'Program Manager', 'Technical Program Manager'], 'total_roles': 6, 'seniority_level': 'Senior', 'companies': []}, 'technical_depth': {'core_technologies': ['Ai For Good Project Framework', 'Amazon Web Services (Aws)', 'Artificial Intelligence (Ai)', 'Artificial Neural Networks', 'Automotive'], 'technology_categories': {'Cloud': ['Amazon Web Services (Aws)', 'Cloud Computing', 'Google Cloud Platform (Gcp)'], 'AI/ML': ['Ai For Good Project Framework', 'Artificial Intelligence (Ai)', 'Client Liaison', 'Fundraising', 'Generative Ai']}, 'expertise_level': 'Expert'}, 'leadership': {'team_size_managed': 'Not specified', 'leadership_roles': [], 'key_achievements': []}, 'education': {'degrees': ['Master of Business Administration (M.B.A.)', 'Bachelor of Technology (B.Tech.)', 'Certificate'], 'institutions': ['London Business School', 'National Institute of Technology Calicut', 'London Business School', 'Import Classes'], 'fields': ['Finance and Accounting: Corporate Finance, Capital Markets and Financing, Credit Risk, Financial Accounting, Managerial Accounting  Strategy & Marketing:  Strategic Management, Discovering Entrepreneurial Opportunities, New Ventures Development, Brand Communications, Brand Marketing, Energy: Markets, Models and Strategies  Economics: Managerial Economics, Understanding International Macroeconomy, World Economy: Problems & Prospects  Leadership: Developing Effective Managers and Organisations, Leadership Skills, Understanding General Management, Data Analytics for Managers, Decision and Risk Analysis, Operations Management', 'Electrical & Electronics Engineering', 'In depth 10 week classroom programme covering valuations, merger and acquisition and project finance.', "Attended import classes' Two-Day Introduction to Data Science Course at London Business School on October 3-4, 2015 - Certificate Number: IDS101502"]}, 'certifications': {'certifications': [], 'authorities': [], 'total_certifications': 0}, 'endorsements': {'top_endorsed_skills': []}, 'content_expertise': {'linkedin_post_topics': [], 'linkedin_article_topics': [], 'medium_expertise': ['Architecture', 'Digital Transformation', 'Innovation', 'Leadership', 'Management', 'Strategy', 'Technology'], 'thought_leadership_areas': ['4417749', 'Infrastructure', 'Strategy', 'Retail', 'Security', 'Deep Learning', 'Cloud', 'Leadership', 'Agile', 'Ai', 'Digital Transformation', 'Architecture', 'Gcp', 'Machine Learning', 'Generative Ai', 'Technology', 'Artificial Intelligence', 'Consulting', 'Ml', 'Neural Networks', 'Innovation', 'Manufacturing', 'Data Analytics']}, 'industry_focus': {'primary_industries': [], 'specializations': []}, 'career_progression': {'career_path': [], 'role_duration': 0, 'growth_trajectory': 'Technical Track'}}}
Profile analysis: {'core_competencies': {'primary_skills': ['Agile Project Management', 'Incident Management, Security Information And Event Management (Siem)', 'Stakeholder Management', 'Solution Architecture', 'P&L Management', 'Mentoring', 'Product Strategy', 'Client Relations', 'Artificial Intelligence (Ai)', 'Delivery'], 'skill_frequency': {'Agile Project Management': 1, 'Incident Management, Security Information And Event Management (Siem)': 1, 'Stakeholder Management': 1, 'Solution Architecture': 1, 'P&L Management': 1, 'Mentoring': 1, 'Product Strategy': 1, 'Client Relations': 1, 'Artificial Intelligence (Ai)': 1, 'Delivery': 1, 'Information Security Management System (Isms)': 1, 'Social Media Marketing': 1, 'Cyber Security Risk': 1, 'Management Consulting': 1, 'Artificial Neural Networks': 1, 'Strategy': 1, 'Project Finance': 1, 'Business Analysis': 1, 'Project Management Office (Pmo)': 1, 'Instrumental Variable': 1, 'Sales Processes': 1, 'Cloud Strategy': 1, 'Senior Stakeholder Management': 1, 'Azure': 1, 'Cloud': 1, 'Technical Project Leadership': 1, 'Requirements Gathering': 1, 'Leadership': 1, 'Business Strategy': 1, 'Contract Negotiation': 1, 'Automotive': 1, 'Digital Transformation': 1, 'Cybersecurity': 1, 'Management': 1, 'Project Management': 1, 'Team Building': 1, 'Business Development': 1, 'Machine Learning': 1, 'Generative Ai': 1, 'Leading Development Teams': 1, 'Change Management': 1, 'Propensity Score Matching': 1, 'Cross-Functional Team Leadership': 1, 'Rfp Design': 1, 'Cybersecurity Tools': 1, 'Supervised Learning': 1, 'Vulnerability Management': 1, 'Problem Solving': 1, 'Ml': 1, 'Vendor Management': 1, 'Client Liaison': 1, 'Cloud Computing': 1, 'Cybersecurity Incident Management': 1, 'Coaching': 1, 'Agile & Waterfall Methodologies': 1, 'Team Management': 1, 'Programme Delivery': 1, 'Amazon Web Services (Aws)': 1, 'Aws': 1, 'Network Architecture': 1, 'Data Analysis': 1, 'Program Delivery Management': 1, 'Fundraising': 1, 'Google Cloud Platform (Gcp)': 1, 'Information Technology Infrastructure': 1, 'Product Development': 1, 'Program Management': 1, 'Causal Inference': 1, 'Planning Budgeting & Forecasting': 1, 'Payments': 1, 'Stakeholder Engagement': 1, 'Ai For Good Project Framework': 1, 'Product Management': 1, 'Exploratory Data Analysis': 1, 'Causality': 1, 'Integration': 1, 'Cybersecurity Auditing': 1, 'Ai': 1, 'Agile': 1, 'Digital Marketing': 1, 'Requirements Analysis': 1, 'Network Security': 1, 'Product Design': 1, 'Program Development': 1, 'Entrepreneurship': 1, 'Google Compute Engine': 1, 'Machine Learning Algorithms': 1, 'Software Project Management': 1, 'Team Leadership': 1, 'It Strategy': 1, 'Project Planning': 1, 'Virtual Machines': 1, 'Software Development': 1, 'Supportive Services': 1, 'Agile Methodologies': 1, 'Scrum': 1, 'Security Strategy': 1, 'Partnerships': 1, 'Technical Support': 1, 'Jupyter Notebooks': 1, 'Program Leadership': 1, 'Project Delivery': 1, 'New Product Validation': 1}, 'source_distribution': {'cv': 7, 'technical_skills': 61, 'soft_skills': 36, 'linkedin_topics': 0, 'medium_topics': 23}}, 'experience_level': {'leadership_roles': ['Director', 'Head', 'Lead', 'Program Manager', 'Technical Program Manager'], 'total_roles': 6, 'seniority_level': 'Senior', 'companies': []}, 'technical_depth': {'core_technologies': ['Ai For Good Project Framework', 'Amazon Web Services (Aws)', 'Artificial Intelligence (Ai)', 'Artificial Neural Networks', 'Automotive'], 'technology_categories': {'Cloud': ['Amazon Web Services (Aws)', 'Cloud Computing', 'Google Cloud Platform (Gcp)'], 'AI/ML': ['Ai For Good Project Framework', 'Artificial Intelligence (Ai)', 'Client Liaison', 'Fundraising', 'Generative Ai']}, 'expertise_level': 'Expert'}, 'leadership': {'team_size_managed': 'Not specified', 'leadership_roles': [], 'key_achievements': []}, 'education': {'degrees': ['Master of Business Administration (M.B.A.)', 'Bachelor of Technology (B.Tech.)', 'Certificate'], 'institutions': ['London Business School', 'National Institute of Technology Calicut', 'London Business School', 'Import Classes'], 'fields': ['Finance and Accounting: Corporate Finance, Capital Markets and Financing, Credit Risk, Financial Accounting, Managerial Accounting  Strategy & Marketing:  Strategic Management, Discovering Entrepreneurial Opportunities, New Ventures Development, Brand Communications, Brand Marketing, Energy: Markets, Models and Strategies  Economics: Managerial Economics, Understanding International Macroeconomy, World Economy: Problems & Prospects  Leadership: Developing Effective Managers and Organisations, Leadership Skills, Understanding General Management, Data Analytics for Managers, Decision and Risk Analysis, Operations Management', 'Electrical & Electronics Engineering', 'In depth 10 week classroom programme covering valuations, merger and acquisition and project finance.', "Attended import classes' Two-Day Introduction to Data Science Course at London Business School on October 3-4, 2015 - Certificate Number: IDS101502"]}, 'certifications': {'certifications': [], 'authorities': [], 'total_certifications': 0}, 'endorsements': {'top_endorsed_skills': []}, 'content_expertise': {'linkedin_post_topics': [], 'linkedin_article_topics': [], 'medium_expertise': ['Architecture', 'Digital Transformation', 'Innovation', 'Leadership', 'Management', 'Strategy', 'Technology'], 'thought_leadership_areas': ['4417749', 'Infrastructure', 'Strategy', 'Retail', 'Security', 'Deep Learning', 'Cloud', 'Leadership', 'Agile', 'Ai', 'Digital Transformation', 'Architecture', 'Gcp', 'Machine Learning', 'Generative Ai', 'Technology', 'Artificial Intelligence', 'Consulting', 'Ml', 'Neural Networks', 'Innovation', 'Manufacturing', 'Data Analytics']}, 'industry_focus': {'primary_industries': [], 'specializations': []}, 'career_progression': {'career_path': [], 'role_duration': 0, 'growth_trajectory': 'Technical Track'}}

Debug: Creating tasks with profile data...
Profile data available: True
Profile data keys: dict_keys(['raw_data', 'analysis'])
[1m[95m# Agent:[00m [1m[92mJob Search Specialist[00m
[95m## Task:[00m [92mSearch for jobs with these criteria:
            Positions: ['Head of Technology', 'Technical Program Manager']
            Locations: ['London']
            Minimum Salary: 130000 GBP
            Contract Types: ['permanent', 'contract'][00m
Making request to http://api.adzuna.com/v1/api/jobs/gb/search/1
Response Status: 200
Total results found: 3
Found 3 jobs for position: Head of Technology
Making request to http://api.adzuna.com/v1/api/jobs/gb/search/1
Response Status: 200
Total results found: 1
Found 1 jobs for position: Technical Program Manager
Total jobs found across all positions: 4


[1m[95m# Agent:[00m [1m[92mJob Search Specialist[00m
[95m## Thought:[00m [92mI need to search for job opportunities that match the specific criteria of positions, locations, minimum salary, and contract types. I will create the search criteria accordingly.[00m
[95m## Using tool:[00m [92mJob Search Tool[00m
[95m## Tool Input:[00m [92m
"{\"criteria\": {\"description\": {\"positions\": [\"Head of Technology\", \"Technical Program Manager\"], \"locations\": [\"London\"], \"minimum_salary\": 130000, \"contract_types\": [\"permanent\", \"contract\"]}}}"[00m
[95m## Tool Output:[00m [92m
[{'title': 'Head of Technology Operations', 'company': 'Ventula Consulting Limited', 'location': 'London, UK', 'area': ['UK', 'London'], 'description': 'Head of Technology Operations Hospitality - London (Hybrid) - 700 per day (Inside IR35) - 3 Months rolling Leading hospitality client now requires a Head of Technology to strategically oversee the day-to-day operations of their technology services. This is a strategic leadership role, which will drive operational excellence, ensure smooth delivery of our client’s services and manage relationships with outsourced managed service providers. Key Responsibilities: Develop a strategic roadmap for te…', 'salary_min': 156000, 'salary_max': 182000, 'url': 'URL not available', 'source': 'Adzuna', 'posted_date': '2024-12-23T01:10:09Z', 'category': 'IT Jobs', 'contract_type': 'contract', 'contract_time': None}, {'title': 'Technical Engineer', 'company': 'La Fosse Associates Ltd', 'location': 'South East London, London', 'area': ['UK', 'London', 'South East London'], 'description': "I'm currently partnered with a client who is looking to add a Technical Engineer to the team. - £500 per day - inside IR35 - 12 months initial with extension - 2/3 days per week in London office (St Pauls) - enjoy being part of a small team, passionate about technology and learning new skills. Role overview The Technical Solutions Developer role is a key position that reports to the Head of Technology Solutions. The role covers a wide variety of technologies and involves working on multiple pro…", 'salary_min': 130000, 'salary_max': 130260, 'url': 'URL not available', 'source': 'Adzuna', 'posted_date': '2025-01-08T03:22:19Z', 'category': 'IT Jobs', 'contract_type': 'contract', 'contract_time': None}, {'title': 'Technical Engineer', 'company': 'La Fosse Associates Ltd', 'location': 'London, UK', 'area': ['UK', 'London'], 'description': "I'm currently partnered with a client who is looking to add a Technical Engineer to the team. - £500 per day - inside IR35 - 12 months initial with extension - 2/3 days per week in London office (St Pauls) - enjoy being part of a small team, passionate about technology and learning new skills. Role overview The Technical Solutions Developer role is a key position that reports to the Head of Technology Solutions. The role covers a wide variety of technologies and involves working on multiple pro…", 'salary_min': 130000, 'salary_max': 130260, 'url': 'URL not available', 'source': 'Adzuna', 'posted_date': '2025-01-07T20:49:57Z', 'category': 'IT Jobs', 'contract_type': 'contract', 'contract_time': None}, {'title': 'Project Manager, Professional Services', 'company': 'Harness', 'location': 'London, UK', 'area': ['UK', 'London'], 'description': 'Harness is a high-growth company that is disrupting the software delivery market. Our mission is to enable the 30 million software developers in the world to deliver code to their users reliably, efficiently, securely and quickly, increasing customers’ pace of innovation while improving the developer experience. We offer solutions for every step of the software delivery lifecycle to build, test, secure, deploy and manage reliability, feature flags and cloud costs. The Harness Software Delivery …', 'salary_min': 160000, 'salary_max': 172000, 'url': 'URL not available', 'source': 'Adzuna', 'posted_date': '2025-01-12T11:58:20Z', 'category': 'IT Jobs', 'contract_type': 'contract', 'contract_time': None}][00m


[1m[95m# Agent:[00m [1m[92mJob Search Specialist[00m
[95m## Final Answer:[00m [92m
1. **Job Title**: Head of Technology Operations  
   **Company**: Ventula Consulting Limited  
   **Location**: London, UK  
   **Description**: Head of Technology Operations Hospitality - London (Hybrid) - 700 per day (Inside IR35) - 3 Months rolling. Leading hospitality client now requires a Head of Technology to strategically oversee the day-to-day operations of their technology services. This is a strategic leadership role, which will drive operational excellence, ensure smooth delivery of our client’s services and manage relationships with outsourced managed service providers.  
   **Minimum Salary**: 156,000 GBP  
   **Maximum Salary**: 182,000 GBP  
   **Contract Type**: Contract  
   **Posted Date**: December 23, 2024  
   **Source**: Adzuna  
   **URL**: URL not available  

2. **Job Title**: Technical Engineer  
   **Company**: La Fosse Associates Ltd  
   **Location**: South East London, London  
   **Description**: I'm currently partnered with a client who is looking to add a Technical Engineer to the team. - £500 per day - inside IR35 - 12 months initial with extension - 2/3 days per week in London office (St Pauls). Enjoy being part of a small team, passionate about technology and learning new skills. Role overview: The Technical Solutions Developer role is a key position that reports to the Head of Technology Solutions. The role covers a wide variety of technologies and involves working on multiple projects.  
   **Minimum Salary**: 130,000 GBP  
   **Maximum Salary**: 130,260 GBP  
   **Contract Type**: Contract  
   **Posted Date**: January 8, 2025  
   **Source**: Adzuna  
   **URL**: URL not available  

3. **Job Title**: Technical Engineer  
   **Company**: La Fosse Associates Ltd  
   **Location**: London, UK  
   **Description**: I'm currently partnered with a client who is looking to add a Technical Engineer to the team. - £500 per day - inside IR35 - 12 months initial with extension - 2/3 days per week in London office (St Pauls). Enjoy being part of a small team, passionate about technology and learning new skills. Role overview: The Technical Solutions Developer role is a key position that reports to the Head of Technology Solutions. The role covers a wide variety of technologies and involves working on multiple projects.  
   **Minimum Salary**: 130,000 GBP  
   **Maximum Salary**: 130,260 GBP  
   **Contract Type**: Contract  
   **Posted Date**: January 7, 2025  
   **Source**: Adzuna  
   **URL**: URL not available  

4. **Job Title**: Project Manager, Professional Services  
   **Company**: Harness  
   **Location**: London, UK  
   **Description**: Harness is a high-growth company that is disrupting the software delivery market. Our mission is to enable the 30 million software developers in the world to deliver code to their users reliably, efficiently, securely, and quickly, increasing customers’ pace of innovation while improving the developer experience. We offer solutions for every step of the software delivery lifecycle to build, test, secure, deploy, and manage reliability, feature flags, and cloud costs.  
   **Minimum Salary**: 160,000 GBP  
   **Maximum Salary**: 172,000 GBP  
   **Contract Type**: Contract  
   **Posted Date**: January 12, 2025  
   **Source**: Adzuna  
   **URL**: URL not available[00m


[1m[95m# Agent:[00m [1m[92mJob Filter Specialist[00m
[95m## Task:[00m [92mFilter and rank the jobs based on:
            1. Apply configured filters
            2. Calculate profile match scores
            3. Sort by relevance and salary
            Use both the job filters and matcher to process the jobs.
            Profile Analysis: {'core_competencies': {'primary_skills': ['Agile Project Management', 'Incident Management, Security Information And Event Management (Siem)', 'Stakeholder Management', 'Solution Architecture', 'P&L Management', 'Mentoring', 'Product Strategy', 'Client Relations', 'Artificial Intelligence (Ai)', 'Delivery'], 'skill_frequency': {'Agile Project Management': 1, 'Incident Management, Security Information And Event Management (Siem)': 1, 'Stakeholder Management': 1, 'Solution Architecture': 1, 'P&L Management': 1, 'Mentoring': 1, 'Product Strategy': 1, 'Client Relations': 1, 'Artificial Intelligence (Ai)': 1, 'Delivery': 1, 'Information Security Management System (Isms)': 1, 'Social Media Marketing': 1, 'Cyber Security Risk': 1, 'Management Consulting': 1, 'Artificial Neural Networks': 1, 'Strategy': 1, 'Project Finance': 1, 'Business Analysis': 1, 'Project Management Office (Pmo)': 1, 'Instrumental Variable': 1, 'Sales Processes': 1, 'Cloud Strategy': 1, 'Senior Stakeholder Management': 1, 'Azure': 1, 'Cloud': 1, 'Technical Project Leadership': 1, 'Requirements Gathering': 1, 'Leadership': 1, 'Business Strategy': 1, 'Contract Negotiation': 1, 'Automotive': 1, 'Digital Transformation': 1, 'Cybersecurity': 1, 'Management': 1, 'Project Management': 1, 'Team Building': 1, 'Business Development': 1, 'Machine Learning': 1, 'Generative Ai': 1, 'Leading Development Teams': 1, 'Change Management': 1, 'Propensity Score Matching': 1, 'Cross-Functional Team Leadership': 1, 'Rfp Design': 1, 'Cybersecurity Tools': 1, 'Supervised Learning': 1, 'Vulnerability Management': 1, 'Problem Solving': 1, 'Ml': 1, 'Vendor Management': 1, 'Client Liaison': 1, 'Cloud Computing': 1, 'Cybersecurity Incident Management': 1, 'Coaching': 1, 'Agile & Waterfall Methodologies': 1, 'Team Management': 1, 'Programme Delivery': 1, 'Amazon Web Services (Aws)': 1, 'Aws': 1, 'Network Architecture': 1, 'Data Analysis': 1, 'Program Delivery Management': 1, 'Fundraising': 1, 'Google Cloud Platform (Gcp)': 1, 'Information Technology Infrastructure': 1, 'Product Development': 1, 'Program Management': 1, 'Causal Inference': 1, 'Planning Budgeting & Forecasting': 1, 'Payments': 1, 'Stakeholder Engagement': 1, 'Ai For Good Project Framework': 1, 'Product Management': 1, 'Exploratory Data Analysis': 1, 'Causality': 1, 'Integration': 1, 'Cybersecurity Auditing': 1, 'Ai': 1, 'Agile': 1, 'Digital Marketing': 1, 'Requirements Analysis': 1, 'Network Security': 1, 'Product Design': 1, 'Program Development': 1, 'Entrepreneurship': 1, 'Google Compute Engine': 1, 'Machine Learning Algorithms': 1, 'Software Project Management': 1, 'Team Leadership': 1, 'It Strategy': 1, 'Project Planning': 1, 'Virtual Machines': 1, 'Software Development': 1, 'Supportive Services': 1, 'Agile Methodologies': 1, 'Scrum': 1, 'Security Strategy': 1, 'Partnerships': 1, 'Technical Support': 1, 'Jupyter Notebooks': 1, 'Program Leadership': 1, 'Project Delivery': 1, 'New Product Validation': 1}, 'source_distribution': {'cv': 7, 'technical_skills': 61, 'soft_skills': 36, 'linkedin_topics': 0, 'medium_topics': 23}}, 'experience_level': {'leadership_roles': ['Director', 'Head', 'Lead', 'Program Manager', 'Technical Program Manager'], 'total_roles': 6, 'seniority_level': 'Senior', 'companies': []}, 'technical_depth': {'core_technologies': ['Ai For Good Project Framework', 'Amazon Web Services (Aws)', 'Artificial Intelligence (Ai)', 'Artificial Neural Networks', 'Automotive'], 'technology_categories': {'Cloud': ['Amazon Web Services (Aws)', 'Cloud Computing', 'Google Cloud Platform (Gcp)'], 'AI/ML': ['Ai For Good Project Framework', 'Artificial Intelligence (Ai)', 'Client Liaison', 'Fundraising', 'Generative Ai']}, 'expertise_level': 'Expert'}, 'leadership': {'team_size_managed': 'Not specified', 'leadership_roles': [], 'key_achievements': []}, 'education': {'degrees': ['Master of Business Administration (M.B.A.)', 'Bachelor of Technology (B.Tech.)', 'Certificate'], 'institutions': ['London Business School', 'National Institute of Technology Calicut', 'London Business School', 'Import Classes'], 'fields': ['Finance and Accounting: Corporate Finance, Capital Markets and Financing, Credit Risk, Financial Accounting, Managerial Accounting  Strategy & Marketing:  Strategic Management, Discovering Entrepreneurial Opportunities, New Ventures Development, Brand Communications, Brand Marketing, Energy: Markets, Models and Strategies  Economics: Managerial Economics, Understanding International Macroeconomy, World Economy: Problems & Prospects  Leadership: Developing Effective Managers and Organisations, Leadership Skills, Understanding General Management, Data Analytics for Managers, Decision and Risk Analysis, Operations Management', 'Electrical & Electronics Engineering', 'In depth 10 week classroom programme covering valuations, merger and acquisition and project finance.', "Attended import classes' Two-Day Introduction to Data Science Course at London Business School on October 3-4, 2015 - Certificate Number: IDS101502"]}, 'certifications': {'certifications': [], 'authorities': [], 'total_certifications': 0}, 'endorsements': {'top_endorsed_skills': []}, 'content_expertise': {'linkedin_post_topics': [], 'linkedin_article_topics': [], 'medium_expertise': ['Architecture', 'Digital Transformation', 'Innovation', 'Leadership', 'Management', 'Strategy', 'Technology'], 'thought_leadership_areas': ['4417749', 'Infrastructure', 'Strategy', 'Retail', 'Security', 'Deep Learning', 'Cloud', 'Leadership', 'Agile', 'Ai', 'Digital Transformation', 'Architecture', 'Gcp', 'Machine Learning', 'Generative Ai', 'Technology', 'Artificial Intelligence', 'Consulting', 'Ml', 'Neural Networks', 'Innovation', 'Manufacturing', 'Data Analytics']}, 'industry_focus': {'primary_industries': [], 'specializations': []}, 'career_progression': {'career_path': [], 'role_duration': 0, 'growth_trajectory': 'Technical Track'}}[00m

Job Filter: Starting job analysis
Processing 4 jobs against profile

Attempting LLM-based analysis...
LLM analysis successful
Using combined LLM and semantic analysis

Analyzing job: Head of Technology Operations
Match score: 0.42
Key matches: Agile Project Management, Incident Management, Stakeholder Management...

Attempting LLM-based analysis...
LLM analysis successful
Using combined LLM and semantic analysis

Analyzing job: Technical Engineer
Match score: 0.35
Key matches: ...

Attempting LLM-based analysis...
LLM analysis successful
Using combined LLM and semantic analysis

Analyzing job: Project Manager, Professional Services
Match score: 0.35
Key matches: Agile Project Management, Stakeholder Management, Solution Architecture...


[1m[95m# Agent:[00m [1m[92mJob Filter Specialist[00m
[95m## Thought:[00m [92mI need to analyze the provided job listings and the candidate profile to filter and rank the available jobs based on match scores and relevance.[00m
[95m## Using tool:[00m [92mJob Filter[00m
[95m## Tool Input:[00m [92m
"{\"jobs\": [{\"description\": \"Head of Technology Operations Hospitality - London (Hybrid) - 700 per day (Inside IR35) - 3 Months rolling. Leading hospitality client now requires a Head of Technology to strategically oversee the day-to-day operations of their technology services. This is a strategic leadership role, which will drive operational excellence, ensure smooth delivery of our client\\u2019s services and manage relationships with outsourced managed service providers.\", \"type\": \"list[dict[str, Any]]\", \"title\": \"Head of Technology Operations\", \"company\": \"Ventula Consulting Limited\", \"location\": \"London, UK\", \"minimum_salary\": 156000, \"maximum_salary\": 182000, \"contract_type\": \"Contract\", \"posted_date\": \"2024-12-23\"}, {\"description\": \"I'm currently partnered with a client who is looking to add a Technical Engineer to the team. - \\u00a3500 per day - inside IR35 - 12 months initial with extension - 2/3 days per week in London office (St Pauls). Enjoy being part of a small team, passionate about technology and learning new skills. Role overview: The Technical Solutions Developer role is a key position that reports to the Head of Technology Solutions. The role covers a wide variety of technologies and involves working on multiple projects.\", \"type\": \"list[dict[str, Any]]\", \"title\": \"Technical Engineer\", \"company\": \"La Fosse Associates Ltd\", \"location\": \"South East London, London\", \"minimum_salary\": 130000, \"maximum_salary\": 130260, \"contract_type\": \"Contract\", \"posted_date\": \"2025-01-08\"}, {\"description\": \"I'm currently partnered with a client who is looking to add a Technical Engineer to the team. - \\u00a3500 per day - inside IR35 - 12 months initial with extension - 2/3 days per week in London office (St Pauls). Enjoy being part of a small team, passionate about technology and learning new skills. Role overview: The Technical Solutions Developer role is a key position that reports to the Head of Technology Solutions. The role covers a wide variety of technologies and involves working on multiple projects.\", \"type\": \"list[dict[str, Any]]\", \"title\": \"Technical Engineer\", \"company\": \"La Fosse Associates Ltd\", \"location\": \"London, UK\", \"minimum_salary\": 130000, \"maximum_salary\": 130260, \"contract_type\": \"Contract\", \"posted_date\": \"2025-01-07\"}, {\"description\": \"Harness is a high-growth company that is disrupting the software delivery market. Our mission is to enable the 30 million software developers in the world to deliver code to their users reliably, efficiently, securely, and quickly, increasing customers\\u2019 pace of innovation while improving the developer experience. We offer solutions for every step of the software delivery lifecycle to build, test, secure, deploy, and manage reliability, feature flags, and cloud costs.\", \"type\": \"list[dict[str, Any]]\", \"title\": \"Project Manager, Professional Services\", \"company\": \"Harness\", \"location\": \"London, UK\", \"minimum_salary\": 160000, \"maximum_salary\": 172000, \"contract_type\": \"Contract\", \"posted_date\": \"2025-01-12\"}], \"profile_analysis\": {\"core_competencies\": {\"primary_skills\": [\"Agile Project Management\", \"Incident Management, Security Information And Event Management (Siem)\", \"Stakeholder Management\", \"Solution Architecture\", \"P&L Management\", \"Mentoring\", \"Product Strategy\", \"Client Relations\", \"Artificial Intelligence (Ai)\", \"Delivery\"], \"skill_frequency\": {\"Agile Project Management\": 1, \"Incident Management, Security Information And Event Management (Siem)\": 1, \"Stakeholder Management\": 1, \"Solution Architecture\": 1, \"P&L Management\": 1, \"Mentoring\": 1, \"Product Strategy\": 1, \"Client Relations\": 1, \"Artificial Intelligence (Ai)\": 1, \"Delivery\": 1}}, \"experience_level\": {\"leadership_roles\": [\"Director\", \"Head\", \"Lead\", \"Program Manager\", \"Technical Program Manager\"], \"total_roles\": 6, \"seniority_level\": \"Senior\"}, \"technical_depth\": {\"core_technologies\": [\"Ai For Good Project Framework\", \"Amazon Web Services (Aws)\", \"Artificial Intelligence (Ai)\"], \"expertise_level\": \"Expert\"}, \"education\": {\"degrees\": [\"Master of Business Administration (M.B.A.)\", \"Bachelor of Technology (B.Tech.)\"]}}}"[00m
[95m## Tool Output:[00m [92m
[
  {
    "description": "Head of Technology Operations Hospitality - London (Hybrid) - 700 per day (Inside IR35) - 3 Months rolling. Leading hospitality client now requires a Head of Technology to strategically oversee the day-to-day operations of their technology services. This is a strategic leadership role, which will drive operational excellence, ensure smooth delivery of our client\u2019s services and manage relationships with outsourced managed service providers.",
    "type": "list[dict[str, Any]]",
    "title": "Head of Technology Operations",
    "company": "Ventula Consulting Limited",
    "location": "London, UK",
    "minimum_salary": 156000,
    "maximum_salary": 182000,
    "contract_type": "Contract",
    "posted_date": "2024-12-23",
    "match_info": {
      "score": 0.42,
      "components": {
        "technical": 0.8,
        "leadership": 0.9,
        "experience": 0.1
      },
      "key_matches": [
        "Agile Project Management",
        "Incident Management",
        "Stakeholder Management",
        "Solution Architecture",
        "P&L Management",
        "Mentoring",
        "Product Strategy",
        "Client Relations",
        "Artificial Intelligence",
        "Delivery"
      ],
      "analysis": {
        "matching_qualifications": [
          "Agile Project Management",
          "Incident Management",
          "Stakeholder Management",
          "Solution Architecture",
          "P&L Management",
          "Mentoring",
          "Product Strategy",
          "Client Relations",
          "Artificial Intelligence",
          "Delivery"
        ],
        "gaps": [
          "Industry Focus",
          "Thought Leadership Areas",
          "Total Roles",
          "Career Path",
          "Key Certifications"
        ],
        "seniority_fit": "Senior level candidate",
        "experience_matches": [],
        "leadership_alignment": "Strong leadership alignment based on previous roles like Director, Head, Lead, Program Manager",
        "technical_alignment": "Strong technical alignment based on expertise in Ai For Good Project Framework, Amazon Web Services, Artificial Intelligence",
        "industry_fit": "Lacks industry focus and thought leadership areas which are important for this role"
      },
      "semantic_scores": {
        "experience_semantic_match": 0.0,
        "content_semantic_match": 0.0,
        "overall_semantic_match": 0.0,
        "matching_experiences": [],
        "matching_posts": []
      }
    },
    "profile_match_score": 0.42
  },
  {
    "description": "I'm currently partnered with a client who is looking to add a Technical Engineer to the team. - \u00a3500 per day - inside IR35 - 12 months initial with extension - 2/3 days per week in London office (St Pauls). Enjoy being part of a small team, passionate about technology and learning new skills. Role overview: The Technical Solutions Developer role is a key position that reports to the Head of Technology Solutions. The role covers a wide variety of technologies and involves working on multiple projects.",
    "type": "list[dict[str, Any]]",
    "title": "Technical Engineer",
    "company": "La Fosse Associates Ltd",
    "location": "South East London, London",
    "minimum_salary": 130000,
    "maximum_salary": 130260,
    "contract_type": "Contract",
    "posted_date": "2025-01-08",
    "match_info": {
      "score": 0.35,
      "components": {
        "technical": 0.2,
        "leadership": 0.8,
        "experience": 0
      },
      "key_matches": [],
      "analysis": {
        "matching_qualifications": [],
        "gaps": [
          "Technical experience in Ai For Good Project Framework, Amazon Web Services (AWS), Artificial Intelligence (AI)",
          "Lack of industry focus and thought leadership areas"
        ],
        "seniority_fit": "Not Applicable",
        "experience_matches": [],
        "leadership_alignment": "Strong",
        "technical_alignment": "Weak",
        "industry_fit": "Not Applicable"
      },
      "semantic_scores": {
        "experience_semantic_match": 0.0,
        "content_semantic_match": 0.0,
        "overall_semantic_match": 0.0,
        "matching_experiences": [],
        "matching_posts": []
      }
    },
    "profile_match_score": 0.35
  },
  {
    "description": "Harness is a high-growth company that is disrupting the software delivery market. Our mission is to enable the 30 million software developers in the world to deliver code to their users reliably, efficiently, securely, and quickly, increasing customers\u2019 pace of innovation while improving the developer experience. We offer solutions for every step of the software delivery lifecycle to build, test, secure, deploy, and manage reliability, feature flags, and cloud costs.",
    "type": "list[dict[str, Any]]",
    "title": "Project Manager, Professional Services",
    "company": "Harness",
    "location": "London, UK",
    "minimum_salary": 160000,
    "maximum_salary": 172000,
    "contract_type": "Contract",
    "posted_date": "2025-01-12",
    "match_info": {
      "score": 0.35,
      "components": {
        "technical": 0.2,
        "leadership": 0.8,
        "experience": 0
      },
      "key_matches": [
        "Agile Project Management",
        "Stakeholder Management",
        "Solution Architecture",
        "Mentoring",
        "Product Strategy",
        "Client Relations",
        "Delivery"
      ],
      "analysis": {
        "matching_qualifications": [
          "Agile Project Management",
          "Stakeholder Management",
          "Solution Architecture",
          "Mentoring",
          "Product Strategy",
          "Client Relations",
          "Delivery"
        ],
        "gaps": [
          "Relevant experience in project management roles",
          "Industry focus and thought leadership"
        ],
        "seniority_fit": "Senior level candidate",
        "experience_matches": [],
        "leadership_alignment": "Strong leadership roles align with the job requirements",
        "technical_alignment": "Some technical expertise in AI and AWS, but not directly aligned with the job requirements",
        "industry_fit": "Lacks industry focus and thought leadership"
      },
      "semantic_scores": {
        "experience_semantic_match": 0.0,
        "content_semantic_match": 0.0,
        "overall_semantic_match": 0.0,
        "matching_experiences": [],
        "matching_posts": []
      }
    },
    "profile_match_score": 0.35
  }
][00m


[1m[95m# Agent:[00m [1m[92mJob Filter Specialist[00m
[95m## Final Answer:[00m [92m
```json
[
  {
    "Job Title": "Head of Technology Operations",
    "Company": "Ventula Consulting Limited",
    "Location": "London, UK",
    "Description": "Head of Technology Operations Hospitality - London (Hybrid) - 700 per day (Inside IR35) - 3 Months rolling. Leading hospitality client now requires a Head of Technology to strategically oversee the day-to-day operations of their technology services. This is a strategic leadership role, which will drive operational excellence, ensure smooth delivery of our client’s services and manage relationships with outsourced managed service providers.",
    "Minimum Salary": "156,000 GBP",
    "Maximum Salary": "182,000 GBP",
    "Contract Type": "Contract",
    "Posted Date": "2024-12-23",
    "Profile Match Score": 0.42,
    "Match Info": {
      "Technical Score": 0.8,
      "Leadership Score": 0.9,
      "Experience Score": 0.1,
      "Key Matches": [
        "Agile Project Management",
        "Incident Management",
        "Stakeholder Management",
        "Solution Architecture",
        "P&L Management",
        "Mentoring",
        "Product Strategy",
        "Client Relations",
        "Artificial Intelligence",
        "Delivery"
      ]
    }
  },
  {
    "Job Title": "Technical Engineer",
    "Company": "La Fosse Associates Ltd",
    "Location": "South East London, London",
    "Description": "I'm currently partnered with a client who is looking to add a Technical Engineer to the team. - £500 per day - inside IR35 - 12 months initial with extension - 2/3 days per week in London office (St Pauls). Enjoy being part of a small team, passionate about technology and learning new skills. Role overview: The Technical Solutions Developer role is a key position that reports to the Head of Technology Solutions. The role covers a wide variety of technologies and involves working on multiple projects.",
    "Minimum Salary": "130,000 GBP",
    "Maximum Salary": "130,260 GBP",
    "Contract Type": "Contract",
    "Posted Date": "2025-01-08",
    "Profile Match Score": 0.35,
    "Match Info": {
      "Technical Score": 0.2,
      "Leadership Score": 0.8,
      "Experience Score": 0,
      "Key Matches": []
    }
  },
  {
    "Job Title": "Project Manager, Professional Services",
    "Company": "Harness",
    "Location": "London, UK",
    "Description": "Harness is a high-growth company that is disrupting the software delivery market. Our mission is to enable the 30 million software developers in the world to deliver code to their users reliably, efficiently, securely, and quickly, increasing customers’ pace of innovation while improving the developer experience. We offer solutions for every step of the software delivery lifecycle to build, test, secure, deploy, and manage reliability, feature flags, and cloud costs.",
    "Minimum Salary": "160,000 GBP",
    "Maximum Salary": "172,000 GBP",
    "Contract Type": "Contract",
    "Posted Date": "2025-01-12",
    "Profile Match Score": 0.35,
    "Match Info": {
      "Technical Score": 0.2,
      "Leadership Score": 0.8,
      "Experience Score": 0,
      "Key Matches": [
        "Agile Project Management",
        "Stakeholder Management",
        "Solution Architecture",
        "Mentoring",
        "Product Strategy",
        "Client Relations",
        "Delivery"
      ]
    }
  }
]
```[00m


[1m[95m# Agent:[00m [1m[92mJob Analysis Specialist[00m
[95m## Task:[00m [92mAnalyze the filtered and ranked jobs:
            1. Top matching positions
            2. Salary ranges
            3. Required skills analysis
            4. Recommendations for profile improvements[00m


[1m[95m# Agent:[00m [1m[92mJob Analysis Specialist[00m
[95m## Final Answer:[00m [92m
### Detailed Analysis of Job Matches

#### 1. Top Matching Positions

1. **Head of Technology Operations - Ventula Consulting Limited**
   - **Location:** London, UK
   - **Salary Range:** £156,000 - £182,000
   - **Profile Match Score:** 0.42
   - **Contract Type:** Contract (Inside IR35)
   - **Match Info:**
     - **Technical Score:** 0.8
     - **Leadership Score:** 0.9
     - **Experience Score:** 0.1

   - **Key Matches:**
     - Agile Project Management
     - Incident Management
     - Stakeholder Management
     - Solution Architecture
     - P&L Management
     - Mentoring
     - Product Strategy
     - Client Relations
     - Artificial Intelligence
     - Delivery

2. **Technical Engineer - La Fosse Associates Ltd**
   - **Location:** South East London
   - **Salary Range:** £130,000 - £130,260
   - **Profile Match Score:** 0.35
   - **Contract Type:** Contract (Inside IR35)
   - **Match Info:**
     - **Technical Score:** 0.2
     - **Leadership Score:** 0.8
     - **Experience Score:** 0

   - **Key Matches:** None identified

3. **Project Manager, Professional Services - Harness**
   - **Location:** London, UK
   - **Salary Range:** £160,000 - £172,000
   - **Profile Match Score:** 0.35
   - **Contract Type:** Contract
   - **Match Info:**
     - **Technical Score:** 0.2
     - **Leadership Score:** 0.8
     - **Experience Score:** 0

   - **Key Matches:**
     - Agile Project Management
     - Stakeholder Management
     - Solution Architecture
     - Mentoring
     - Product Strategy
     - Client Relations
     - Delivery

#### 2. Salary Ranges

- The salary ranges for these positions are competitive, with the Head of Technology Operations offering the highest potential, reaching up to £182,000.
- The Technical Engineer position has a tighter range, indicating a possible structured pay scale, while the Project Manager position falls into a similar competitive range as the Head of Technology Operations but does not exceed it.

#### 3. Required Skills Analysis

- **Head of Technology Operations:** 
  - Demonstrates a strong requirement for leadership and strategic procedure oversight, with high expectations for technology operations management in a hospitality context.
  - Skills such as Agile Project Management and Solution Architecture are pivotal for success in this role.
  
- **Technical Engineer:**
  - Skills required are less defined in the listing, indicating a more open-ended role focused on adaptability and technology engagement.
  - However, the low scores in technical and experience suggest a misalignment either in specificity or candidate qualifications needed for current tech trends.
  
- **Project Manager, Professional Services:**
  - Indicates essential testing and delivery phase understands with client interactions, suggesting a need for a strong background in software delivery environments.
  - Matches indicating Agile methodologies and stakeholder handling highlight a growing trend towards flexibility in project management.

#### 4. Recommendations for Profile Improvements

1. **Enhance Technical Skills:**
   - For both the Technical Engineer and Project Manager roles, it would be beneficial to improve technical expertise in areas pertinent to the software delivery lifecycle, such as cloud services, incident management, and relevant programming languages.

2. **Leadership Development:**
   - Given the high weight placed on leadership scores, consider leadership roles or courses that foster these capabilities. Engaging in professional development or certification programs can track achievements that improve their leadership profile.

3. **Broadening Project Management Experience:**
   - Accumulate experience in agile settings or technological product deliveries. This can be achieved through project involvement that specifically highlights these methodologies.

4. **Mentoring Experience:**
   - Since mentoring is a key match, actively seek roles that allow for mentoring or leading teams to boost this profile metric significantly, which would enhance leadership perceptions to potential employers.

5. **Networking:**
   - Engage in industry networking events focused on technology management or software delivery. This can facilitate connections that further bolster opportunities in desired roles and receive feedback on skills needed.

By undertaking these recommended actions, potential candidates can greatly improve their match for these top positions, ideally increasing their Profile Match Scores and enhancing their overall career trajectories.[00m



Job Search Results:
### Detailed Analysis of Job Matches

#### 1. Top Matching Positions

1. **Head of Technology Operations - Ventula Consulting Limited**
   - **Location:** London, UK
   - **Salary Range:** £156,000 - £182,000
   - **Profile Match Score:** 0.42
   - **Contract Type:** Contract (Inside IR35)
   - **Match Info:**
     - **Technical Score:** 0.8
     - **Leadership Score:** 0.9
     - **Experience Score:** 0.1

   - **Key Matches:**
     - Agile Project Management
     - Incident Management
     - Stakeholder Management
     - Solution Architecture
     - P&L Management
     - Mentoring
     - Product Strategy
     - Client Relations
     - Artificial Intelligence
     - Delivery

2. **Technical Engineer - La Fosse Associates Ltd**
   - **Location:** South East London
   - **Salary Range:** £130,000 - £130,260
   - **Profile Match Score:** 0.35
   - **Contract Type:** Contract (Inside IR35)
   - **Match Info:**
     - **Technical Score:** 0.2
     - **Leadership Score:** 0.8
     - **Experience Score:** 0

   - **Key Matches:** None identified

3. **Project Manager, Professional Services - Harness**
   - **Location:** London, UK
   - **Salary Range:** £160,000 - £172,000
   - **Profile Match Score:** 0.35
   - **Contract Type:** Contract
   - **Match Info:**
     - **Technical Score:** 0.2
     - **Leadership Score:** 0.8
     - **Experience Score:** 0

   - **Key Matches:**
     - Agile Project Management
     - Stakeholder Management
     - Solution Architecture
     - Mentoring
     - Product Strategy
     - Client Relations
     - Delivery

#### 2. Salary Ranges

- The salary ranges for these positions are competitive, with the Head of Technology Operations offering the highest potential, reaching up to £182,000.
- The Technical Engineer position has a tighter range, indicating a possible structured pay scale, while the Project Manager position falls into a similar competitive range as the Head of Technology Operations but does not exceed it.

#### 3. Required Skills Analysis

- **Head of Technology Operations:** 
  - Demonstrates a strong requirement for leadership and strategic procedure oversight, with high expectations for technology operations management in a hospitality context.
  - Skills such as Agile Project Management and Solution Architecture are pivotal for success in this role.
  
- **Technical Engineer:**
  - Skills required are less defined in the listing, indicating a more open-ended role focused on adaptability and technology engagement.
  - However, the low scores in technical and experience suggest a misalignment either in specificity or candidate qualifications needed for current tech trends.
  
- **Project Manager, Professional Services:**
  - Indicates essential testing and delivery phase understands with client interactions, suggesting a need for a strong background in software delivery environments.
  - Matches indicating Agile methodologies and stakeholder handling highlight a growing trend towards flexibility in project management.

#### 4. Recommendations for Profile Improvements

1. **Enhance Technical Skills:**
   - For both the Technical Engineer and Project Manager roles, it would be beneficial to improve technical expertise in areas pertinent to the software delivery lifecycle, such as cloud services, incident management, and relevant programming languages.

2. **Leadership Development:**
   - Given the high weight placed on leadership scores, consider leadership roles or courses that foster these capabilities. Engaging in professional development or certification programs can track achievements that improve their leadership profile.

3. **Broadening Project Management Experience:**
   - Accumulate experience in agile settings or technological product deliveries. This can be achieved through project involvement that specifically highlights these methodologies.

4. **Mentoring Experience:**
   - Since mentoring is a key match, actively seek roles that allow for mentoring or leading teams to boost this profile metric significantly, which would enhance leadership perceptions to potential employers.

5. **Networking:**
   - Engage in industry networking events focused on technology management or software delivery. This can facilitate connections that further bolster opportunities in desired roles and receive feedback on skills needed.

By undertaking these recommended actions, potential candidates can greatly improve their match for these top positions, ideally increasing their Profile Match Scores and enhancing their overall career trajectories.
